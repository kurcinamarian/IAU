{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4110b7d-d5cd-4e78-b618-841eac6bf638",
   "metadata": {},
   "source": [
    "# Intelligent Data Analysis Project\n",
    "### Matej Bebej (50%), Marian Kurcina (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ef770-04f0-4a1b-8968-a093a397df5b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Assignment\n",
    "- Phase 2 - Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85d8a5-61a7-4bb5-ae86-c78db03585c6",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51d95b-e2d3-416b-bd59-4fb793c70703",
   "metadata": {},
   "source": [
    "Oxygen saturation is a key indicator of the proper functioning of the respiratory and circulatory systems. When its value drops to a critically low level, it may indicate life-threatening conditions such as hypoxemia, respiratory failure, or severe infections. In such cases, immediate intervention is essential. Traditional monitoring is performed using pulse oximeters, which, however, can be affected by noise, motion artifacts, or may have limitations in certain clinical situations.\n",
    "\n",
    "Modern machine learning–based approaches offer the possibility to estimate and predict critical oxygen saturation values with higher accuracy (critical oxygen saturation estimation). Models can utilize multimodal data, such as heart rate, respiratory rate, blood pressure, or sensor signals. By being trained on diverse datasets, it is possible to identify early warning signs of desaturation, filter out noise, and provide timely alerts even before oxygen saturation drops below a safe threshold.\n",
    "\n",
    "The goal of this assignment is to become familiar with the issue of oxygen saturation monitoring, understand the contribution of artificial intelligence, and design a solution that could improve critical care and reduce risks associated with undiagnosed hypoxemia.\n",
    "\n",
    "Each pair of students will work with an assigned dataset starting from Week 2. Your task is to predict the dependent variable “oximetry” (the predicted variable) using machine learning methods. In doing so, you will need to deal with various issues present in the data, such as inconsistent formats, missing values, outliers, and others.\n",
    "\n",
    "The expected outcomes of the project are:\n",
    "\n",
    "- the best-performing machine learning model, and\n",
    "\n",
    "- a data pipeline for building it from the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce20ec-579b-4e1f-bd06-44d5324f812e",
   "metadata": {},
   "source": [
    "# Phase 2 – Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe3e813-415c-4498-8873-635a77c81807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateparser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "observation = pd.read_csv(\"dataset/observation.csv\", sep='\\t')\n",
    "patient = pd.read_csv(\"dataset/patient.csv\", sep='\\t')\n",
    "station = pd.read_csv(\"dataset/station.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c9876-3f96-4afe-bbfe-d39cb93b99a2",
   "metadata": {},
   "source": [
    "In this phase, you are expected to carry out data preprocessing for machine learning. The result should be a dataset (CSV or TSV), where each observation is described by one row.\n",
    "Since scikit-learn only works with numerical data, something must be done with the non-numerical data.\n",
    "\n",
    "Ensure the preprocessing is reproducible on both the training and test datasets, so that you can repeat the process multiple times as needed (iteratively).\n",
    "\n",
    "Because preprocessing can change the shape and characteristics of the data, you may need to perform EDA (Exploratory Data Analysis) again as necessary. These techniques will not be graded again, but document any changes in the chosen methods.\n",
    "You can solve data-related issues iteratively across all phases, as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b823b-ecb7-467d-abf2-2af9d3700a6c",
   "metadata": {},
   "source": [
    "## 2.1 Implementation of Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bb233-dc76-4391-b893-beeb833cb867",
   "metadata": {},
   "source": [
    "### A - Train–Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db092e00-b43f-4e5d-b52a-c3d53baa35ab",
   "metadata": {},
   "source": [
    "Split the data into training and test sets according to your predefined ratio. Continue working only with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc43d6c4-18be-4e11-bb8e-cae030f9a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (9685, 23)\n",
      "test shape: (2422, 23)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(\n",
    "    observation, \n",
    "    test_size=0.2,        # 20% for testing\n",
    "    random_state=42,      # reproducibility\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"train shape:\", train.shape)\n",
    "print(\"test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef1b03-c01e-4000-91bf-32d9fab0d755",
   "metadata": {},
   "source": [
    "We split data into train and test parts, with 20% of observation data being test data () and 80% being train data ()."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e91b3d40-b7ea-4f1f-9d7a-4752bf3edaf0",
   "metadata": {},
   "source": [
    "First we preprocess train data by: \n",
    "- enforcing schemas on each data frame\n",
    "- removing latitude and longitude columns from observation since they are not medical data and have no corelation to oxymetry (as we learned in First Phase of the project - EDA)\n",
    "- removing logical outliers - values which are outside of acceptable range\n",
    "- removing records which have oxymetry missing \n",
    "- removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "041761b8-351a-4dc7-8227-333ea88209b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_schema = {\n",
    "    'location':'string',\n",
    "    'code':'string',\n",
    "    'revision':'date',\n",
    "    'station':'string',\n",
    "    'latitude':'float',\n",
    "    'longitude':'float',\n",
    "}\n",
    "observation_schema = {\n",
    "    'SpO₂':'float',\n",
    "    'HR':'float',\n",
    "    'PI':'float',\n",
    "    'RR':'float',\n",
    "    'EtCO₂':'float',\n",
    "    'FiO₂':'float',\n",
    "    'PRV':'float',\n",
    "    'BP':'float',\n",
    "    'Skin Temperature':'float',\n",
    "    'Motion/Activity index':'float',\n",
    "    'PVI':'float',\n",
    "    'Hb level':'float',\n",
    "    'SV':'float',\n",
    "    'CO':'float',\n",
    "    'Blood Flow Index':'float',\n",
    "    'PPG waveform features':'float',\n",
    "    'Signal Quality Index':'float',\n",
    "    'Respiratory effort':'float',\n",
    "    'O₂ extraction ratio':'float',\n",
    "    'SNR':'float',\n",
    "    'oximetry':'int',\n",
    "    'latitude':'float',\n",
    "    'longitude':'float'\n",
    "}\n",
    "patient_schema = {\n",
    "    'residence':'string',              \n",
    "    'current_location':'string',    \n",
    "    'blood_group':'string',          \n",
    "    'job':'string',                 \n",
    "    'mail':'string',                \n",
    "    'user_id':'int',             \n",
    "    'birthdate':'date',           \n",
    "    'company':'string',             \n",
    "    'name':'string',                \n",
    "    'username':'string',            \n",
    "    'ssn':'string',                 \n",
    "    'registration':'date',        \n",
    "    'station_ID':'int'            \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a07760-5afc-43d2-9bf3-69ca14ac6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnforceSchema(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, schema=None):\n",
    "        self.schema = schema\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.schema is None:\n",
    "            return X\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        for col, col_type in self.schema.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            if col_type == 'int':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce').astype('Int64')\n",
    "\n",
    "            elif col_type == 'float':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)\n",
    "\n",
    "            elif col_type == 'numeric':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "            elif col_type == 'date':\n",
    "                X[col] = X[col].apply(\n",
    "                    lambda x: dateparser.parse(str(x))\n",
    "                    if pd.notnull(x) else pd.NaT\n",
    "                )\n",
    "\n",
    "            elif col_type == 'string':\n",
    "                X[col] = X[col].astype(str)\n",
    "                X[col] = X[col].replace('nan', np.nan)\n",
    "\n",
    "            elif col_type == 'bool':\n",
    "                X[col] = (\n",
    "                    X[col]\n",
    "                    .astype(str)\n",
    "                    .str.lower()\n",
    "                    .map({'true': True, 'false': False, '1': True, '0': False})\n",
    "                )\n",
    "\n",
    "            elif col_type == 'category':\n",
    "                X[col] = X[col].astype('category')\n",
    "\n",
    "    \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f957a35-c33d-41e2-bd14-d53f67dc7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ranges = {\n",
    "    'SpO₂': (95, 100),\n",
    "    'HR': (60, 100),\n",
    "    'PI': (0.2, 20),\n",
    "    'RR': (12, 20),\n",
    "    'EtCO₂': (35, 45),\n",
    "    'FiO₂': (21, 100),\n",
    "    'PRV': (20, 200),\n",
    "    'BP': (60, 120),\n",
    "    'Skin Temperature': (33, 38),\n",
    "    'Motion/Activity index': None,\n",
    "    'PVI': (10, 20),\n",
    "    'Hb level': (12, 18),\n",
    "    'SV': (60, 100),\n",
    "    'CO': (4, 8),\n",
    "    'Blood Flow Index': None,\n",
    "    'PPG waveform features': None,\n",
    "    'Signal Quality Index': (0, 100),\n",
    "    'Respiratory effort': None,\n",
    "    'O₂ extraction ratio': (0.2, 3),\n",
    "    'SNR': (20, 40)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09f2f6c-4e87-44e2-845b-fe707f9052d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnforceValueRanges(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ranges=None):\n",
    "        self.ranges = ranges\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.ranges is None:\n",
    "            return X\n",
    "\n",
    "        for col, valid_range in self.ranges.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            if valid_range is None:\n",
    "                continue  \n",
    "\n",
    "            low, high = valid_range\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "            mask = ~X[col].between(low, high, inclusive='both')\n",
    "            X.loc[mask, col] = np.nan\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab7e746-79f1-4f34-9d5a-a37976760c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseLocation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column='location'):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.column not in X.columns:\n",
    "            return X\n",
    "        \n",
    "        split_cols = X[self.column].astype(str).str.split('/', n=1, expand=True)\n",
    "        X['continent'] = split_cols[0].str.strip()\n",
    "        X['city'] = split_cols[1].str.strip()\n",
    "        X = X.drop(columns=[self.column], errors='ignore')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a02ac5c-7387-467a-908e-971d6b4ad0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns or []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9b8ce7d-7bb3-4f70-a67d-c917e7a00253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDuplicates(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f44b8e5-a292-4555-9bc6-3bbc87747dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNA(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, how='any', subset=None):\n",
    "        self.how = how\n",
    "        self.subset = subset\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.dropna(how=self.how, subset=self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b080dbc-e1cd-43a8-8b5f-c69b335ee41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_22272\\1183780406.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[col] = X[col].replace('nan', np.nan)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>continent</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES</td>\n",
       "      <td>37.35813</td>\n",
       "      <td>-6.03731</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO</td>\n",
       "      <td>7.83389</td>\n",
       "      <td>-72.47417</td>\n",
       "      <td>America</td>\n",
       "      <td>Bogota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN</td>\n",
       "      <td>26.44931</td>\n",
       "      <td>91.61356</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Kolkata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>37.95143</td>\n",
       "      <td>-91.77127</td>\n",
       "      <td>America</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>48.21644</td>\n",
       "      <td>9.02596</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code  latitude  longitude continent     city\n",
       "0   ES  37.35813   -6.03731    Europe   Madrid\n",
       "1   CO   7.83389  -72.47417   America   Bogota\n",
       "2   IN  26.44931   91.61356      Asia  Kolkata\n",
       "3   US  37.95143  -91.77127   America  Chicago\n",
       "4   DE  48.21644    9.02596    Europe   Berlin"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "patient_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=patient_schema)),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "observation = observation_pipeline.fit_transform(train)\n",
    "station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "patient = patient_pipeline.fit_transform(pd.read_csv(\"dataset/patient.csv\", sep='\\t'))\n",
    "\n",
    "station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae639950-ff60-4e42-8b56-ead60d49b0d5",
   "metadata": {},
   "source": [
    "### B - Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb66f96-d6da-4c4d-8eb3-573907f45926",
   "metadata": {},
   "source": [
    "Transform the data into a format suitable for machine learning, i.e. each observation must be described by one row, and each attribute must be numeric.\n",
    "Iteratively integrate preprocessing steps from Phase 1 as part of a unified process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc8fab-112d-45d9-9277-9bad261b9746",
   "metadata": {},
   "source": [
    "For observation dataset there are no string or category features, therefore we will show encoding on station dataset. In pipeline we will still call encoding but it will have no effect.\n",
    "\n",
    "We also ignore revision column and station column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a764906-9ae6-42f2-b6f4-0e90ba24edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_22272\\1183780406.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[col] = X[col].replace('nan', np.nan)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat__continent_Africa</th>\n",
       "      <th>cat__continent_America</th>\n",
       "      <th>cat__continent_Asia</th>\n",
       "      <th>cat__continent_Atlantic</th>\n",
       "      <th>cat__continent_Australia</th>\n",
       "      <th>cat__continent_Europe</th>\n",
       "      <th>cat__continent_Indian</th>\n",
       "      <th>cat__continent_Pacific</th>\n",
       "      <th>cat__city_Abidjan</th>\n",
       "      <th>cat__city_Accra</th>\n",
       "      <th>...</th>\n",
       "      <th>cat__code_UA</th>\n",
       "      <th>cat__code_US</th>\n",
       "      <th>cat__code_UY</th>\n",
       "      <th>cat__code_UZ</th>\n",
       "      <th>cat__code_VE</th>\n",
       "      <th>cat__code_VU</th>\n",
       "      <th>cat__code_YE</th>\n",
       "      <th>cat__code_ZA</th>\n",
       "      <th>remainder__latitude</th>\n",
       "      <th>remainder__longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.35813</td>\n",
       "      <td>-6.03731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.83389</td>\n",
       "      <td>-72.47417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.44931</td>\n",
       "      <td>91.61356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.95143</td>\n",
       "      <td>-91.77127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.21644</td>\n",
       "      <td>9.02596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 238 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat__continent_Africa  cat__continent_America  cat__continent_Asia  \\\n",
       "0                    0.0                     0.0                  0.0   \n",
       "1                    0.0                     1.0                  0.0   \n",
       "2                    0.0                     0.0                  1.0   \n",
       "3                    0.0                     1.0                  0.0   \n",
       "4                    0.0                     0.0                  0.0   \n",
       "\n",
       "   cat__continent_Atlantic  cat__continent_Australia  cat__continent_Europe  \\\n",
       "0                      0.0                       0.0                    1.0   \n",
       "1                      0.0                       0.0                    0.0   \n",
       "2                      0.0                       0.0                    0.0   \n",
       "3                      0.0                       0.0                    0.0   \n",
       "4                      0.0                       0.0                    1.0   \n",
       "\n",
       "   cat__continent_Indian  cat__continent_Pacific  cat__city_Abidjan  \\\n",
       "0                    0.0                     0.0                0.0   \n",
       "1                    0.0                     0.0                0.0   \n",
       "2                    0.0                     0.0                0.0   \n",
       "3                    0.0                     0.0                0.0   \n",
       "4                    0.0                     0.0                0.0   \n",
       "\n",
       "   cat__city_Accra  ...  cat__code_UA  cat__code_US  cat__code_UY  \\\n",
       "0              0.0  ...           0.0           0.0           0.0   \n",
       "1              0.0  ...           0.0           0.0           0.0   \n",
       "2              0.0  ...           0.0           0.0           0.0   \n",
       "3              0.0  ...           0.0           1.0           0.0   \n",
       "4              0.0  ...           0.0           0.0           0.0   \n",
       "\n",
       "   cat__code_UZ  cat__code_VE  cat__code_VU  cat__code_YE  cat__code_ZA  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           0.0           0.0           0.0           0.0   \n",
       "3           0.0           0.0           0.0           0.0           0.0   \n",
       "4           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   remainder__latitude  remainder__longitude  \n",
       "0             37.35813              -6.03731  \n",
       "1              7.83389             -72.47417  \n",
       "2             26.44931              91.61356  \n",
       "3             37.95143             -91.77127  \n",
       "4             48.21644               9.02596  \n",
       "\n",
       "[5 rows x 238 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, [])\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code'])\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "patient_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=patient_schema)),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['blood_group'])\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "observation = observation_pipeline.fit_transform(train)\n",
    "station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "patient = patient_pipeline.fit_transform(pd.read_csv(\"dataset/patient.csv\", sep='\\t'))\n",
    "\n",
    "station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0307e7-086a-42b6-a078-9a01d1619b39",
   "metadata": {},
   "source": [
    "### C - Feature Scaling and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c29ca-ac4a-4bed-8aca-721c4916c769",
   "metadata": {},
   "source": [
    "Transform the dataset attributes for machine learning using at least the following techniques:\n",
    "\n",
    "- Scaling (2 techniques)\n",
    "- Transformers (2 techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b512-ae65-4923-b382-c2cba6c23ceb",
   "metadata": {},
   "source": [
    "### D - Justification and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d28cc5-25af-4025-b8ee-366d9461eb9c",
   "metadata": {},
   "source": [
    "Justify your choices/decisions for implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9339e-5c24-414f-858d-500c944d687d",
   "metadata": {},
   "source": [
    "## 2.2 Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7379fb-af8d-4059-b891-aa7a5472f972",
   "metadata": {},
   "source": [
    "### A - Identification of Informative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee4daa-9b57-47d3-b04f-135ce3aaa006",
   "metadata": {},
   "source": [
    "Identify which attributes (features) in your data are informative with respect to the target variable (use at least 3 techniques and compare their results)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa483780-560a-4e32-836b-05c3d640d417",
   "metadata": {},
   "source": [
    "### B - Ranking of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b331286-4b6f-4e33-b219-0ec53994f5a4",
   "metadata": {},
   "source": [
    "Rank the identified features by importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec93b5-049e-40cb-940c-671d53552f10",
   "metadata": {},
   "source": [
    "### C - Justification and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9de70-b409-45bf-a9e7-24e22afdfcdf",
   "metadata": {},
   "source": [
    "Justify your choices/decisions for implementation (i.e., provide documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff557b-b0d8-45dd-939b-f0c7e55a3eab",
   "metadata": {},
   "source": [
    "## 2.3 Reproducibility of Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190199a-1b7d-41fc-9d50-62918d367384",
   "metadata": {},
   "source": [
    "### Code Generalization for Reuse and Pipeline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7f3b3-fd34-4fe9-ae1b-d57d0a816d9c",
   "metadata": {},
   "source": [
    "Modify your preprocessing code for the training dataset so that it can be reused without further modifications to preprocess the test dataset in a machine learning context. Use the sklearn.pipeline functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
