{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4110b7d-d5cd-4e78-b618-841eac6bf638",
   "metadata": {},
   "source": [
    "# Intelligent Data Analysis Project\n",
    "### Matej Bebej (50%), Marian Kurcina (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ef770-04f0-4a1b-8968-a093a397df5b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Assignment\n",
    "- Phase 2 - Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85d8a5-61a7-4bb5-ae86-c78db03585c6",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51d95b-e2d3-416b-bd59-4fb793c70703",
   "metadata": {},
   "source": [
    "Oxygen saturation is a key indicator of the proper functioning of the respiratory and circulatory systems. When its value drops to a critically low level, it may indicate life-threatening conditions such as hypoxemia, respiratory failure, or severe infections. In such cases, immediate intervention is essential. Traditional monitoring is performed using pulse oximeters, which, however, can be affected by noise, motion artifacts, or may have limitations in certain clinical situations.\n",
    "\n",
    "Modern machine learning–based approaches offer the possibility to estimate and predict critical oxygen saturation values with higher accuracy (critical oxygen saturation estimation). Models can utilize multimodal data, such as heart rate, respiratory rate, blood pressure, or sensor signals. By being trained on diverse datasets, it is possible to identify early warning signs of desaturation, filter out noise, and provide timely alerts even before oxygen saturation drops below a safe threshold.\n",
    "\n",
    "The goal of this assignment is to become familiar with the issue of oxygen saturation monitoring, understand the contribution of artificial intelligence, and design a solution that could improve critical care and reduce risks associated with undiagnosed hypoxemia.\n",
    "\n",
    "Each pair of students will work with an assigned dataset starting from Week 2. Your task is to predict the dependent variable “oximetry” (the predicted variable) using machine learning methods. In doing so, you will need to deal with various issues present in the data, such as inconsistent formats, missing values, outliers, and others.\n",
    "\n",
    "The expected outcomes of the project are:\n",
    "\n",
    "- the best-performing machine learning model, and\n",
    "\n",
    "- a data pipeline for building it from the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce20ec-579b-4e1f-bd06-44d5324f812e",
   "metadata": {},
   "source": [
    "# Phase 2 – Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe3e813-415c-4498-8873-635a77c81807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateparser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer,PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_regression\n",
    "observation = pd.read_csv(\"dataset/observation.csv\", sep='\\t')\n",
    "patient = pd.read_csv(\"dataset/patient.csv\", sep='\\t')\n",
    "station = pd.read_csv(\"dataset/station.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c9876-3f96-4afe-bbfe-d39cb93b99a2",
   "metadata": {},
   "source": [
    "In this phase, you are expected to carry out data preprocessing for machine learning. The result should be a dataset (CSV or TSV), where each observation is described by one row.\n",
    "Since scikit-learn only works with numerical data, something must be done with the non-numerical data.\n",
    "\n",
    "Ensure the preprocessing is reproducible on both the training and test datasets, so that you can repeat the process multiple times as needed (iteratively).\n",
    "\n",
    "Because preprocessing can change the shape and characteristics of the data, you may need to perform EDA (Exploratory Data Analysis) again as necessary. These techniques will not be graded again, but document any changes in the chosen methods.\n",
    "You can solve data-related issues iteratively across all phases, as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b823b-ecb7-467d-abf2-2af9d3700a6c",
   "metadata": {},
   "source": [
    "## 2.1 Implementation of Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bb233-dc76-4391-b893-beeb833cb867",
   "metadata": {},
   "source": [
    "### A - Train–Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db092e00-b43f-4e5d-b52a-c3d53baa35ab",
   "metadata": {},
   "source": [
    "Split the data into training and test sets according to your predefined ratio. Continue working only with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbb841f-4181-4f0f-b03a-64305bcc5b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates: 1\n"
     ]
    }
   ],
   "source": [
    "before = len(observation)\n",
    "observation = observation.drop_duplicates()\n",
    "after = len(observation)\n",
    "print(\"Removed duplicates:\", before - after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc43d6c4-18be-4e11-bb8e-cae030f9a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (9684, 22)\n",
      "train shape: (9684,)\n",
      "test shape: (2422, 22)\n",
      "test shape: (2422,)\n"
     ]
    }
   ],
   "source": [
    "X = observation.drop(columns=[\"oximetry\"])\n",
    "y = observation[\"oximetry\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"train shape:\", X_train.shape)\n",
    "print(\"train shape:\", y_train.shape)\n",
    "print(\"test shape:\", X_test.shape)\n",
    "print(\"test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef1b03-c01e-4000-91bf-32d9fab0d755",
   "metadata": {},
   "source": [
    "We split data into train and test parts, with 20% of observation data being test data () and 80% being train data ()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763ed48-63b9-476b-b59c-2b2432f8dd37",
   "metadata": {},
   "source": [
    "First we preprocess train data by: \n",
    "- enforcing schemas on each data frame\n",
    "- removing latitude and longitude columns from observation since they are not medical data and have no corelation to oxymetry (as we learned in First Phase of the project - EDA)\n",
    "- removing logical outliers - values which are outside of acceptable range\n",
    "- removing records which have oxymetry missing \n",
    "- removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ae9c2-d526-4776-a642-4e80fc7a85a7",
   "metadata": {},
   "source": [
    "We create a schema - expected format for each dataset and then we check if this format is present in the dataset, if some element is not same as refered in schema we try to cast the content to the correct type, if that is not possible we insert NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041761b8-351a-4dc7-8227-333ea88209b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_schema = {\n",
    "    'location':'string',\n",
    "    'code':'string',\n",
    "    'revision':'date',\n",
    "    'station':'string',\n",
    "    'latitude':'float',\n",
    "    'longitude':'float',\n",
    "}\n",
    "observation_schema = {\n",
    "    'SpO₂':'float',\n",
    "    'HR':'float',\n",
    "    'PI':'float',\n",
    "    'RR':'float',\n",
    "    'EtCO₂':'float',\n",
    "    'FiO₂':'float',\n",
    "    'PRV':'float',\n",
    "    'BP':'float',\n",
    "    'Skin Temperature':'float',\n",
    "    'Motion/Activity index':'float',\n",
    "    'PVI':'float',\n",
    "    'Hb level':'float',\n",
    "    'SV':'float',\n",
    "    'CO':'float',\n",
    "    'Blood Flow Index':'float',\n",
    "    'PPG waveform features':'float',\n",
    "    'Signal Quality Index':'float',\n",
    "    'Respiratory effort':'float',\n",
    "    'O₂ extraction ratio':'float',\n",
    "    'SNR':'float',\n",
    "    'oximetry':'int',\n",
    "    'latitude':'float',\n",
    "    'longitude':'float'\n",
    "}\n",
    "patient_schema = {\n",
    "    'residence':'string',              \n",
    "    'current_location':'string',    \n",
    "    'blood_group':'string',          \n",
    "    'job':'string',                 \n",
    "    'mail':'string',                \n",
    "    'user_id':'int',             \n",
    "    'birthdate':'date',           \n",
    "    'company':'string',             \n",
    "    'name':'string',                \n",
    "    'username':'string',            \n",
    "    'ssn':'string',                 \n",
    "    'registration':'date',        \n",
    "    'station_ID':'int'            \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a07760-5afc-43d2-9bf3-69ca14ac6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnforceSchema(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, schema=None):\n",
    "        self.schema = schema\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.schema is None:\n",
    "            return X\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        for col, col_type in self.schema.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            if col_type == 'int':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce').astype('Int64')\n",
    "\n",
    "            elif col_type == 'float':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)\n",
    "\n",
    "            elif col_type == 'numeric':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "            elif col_type == 'date':\n",
    "                X[col] = X[col].apply(\n",
    "                    lambda x: dateparser.parse(str(x))\n",
    "                    if pd.notnull(x) else pd.NaT\n",
    "                )\n",
    "\n",
    "            elif col_type == 'string':\n",
    "                X[col] = X[col].astype(str)\n",
    "                X[col] = X[col].replace('nan', np.nan)\n",
    "\n",
    "            elif col_type == 'bool':\n",
    "                X[col] = (\n",
    "                    X[col]\n",
    "                    .astype(str)\n",
    "                    .str.lower()\n",
    "                    .map({'true': True, 'false': False, '1': True, '0': False})\n",
    "                )\n",
    "\n",
    "            elif col_type == 'category':\n",
    "                X[col] = X[col].astype('category')\n",
    "\n",
    "    \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ed2be8-58a5-44ea-9e46-6c46143fb371",
   "metadata": {},
   "source": [
    "In observation dataset we have expected ranges for each attribute, we check if all values are within the range, if not we push the value to the edge of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f957a35-c33d-41e2-bd14-d53f67dc7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ranges = {\n",
    "    'SpO₂': (95, 100),\n",
    "    'HR': (60, 100),\n",
    "    'PI': (0.2, 20),\n",
    "    'RR': (12, 20),\n",
    "    'EtCO₂': (35, 45),\n",
    "    'FiO₂': (21, 100),\n",
    "    'PRV': (20, 200),\n",
    "    'BP': (60, 120),\n",
    "    'Skin Temperature': (33, 38),\n",
    "    'Motion/Activity index': None,\n",
    "    'PVI': (10, 20),\n",
    "    'Hb level': (12, 18),\n",
    "    'SV': (60, 100),\n",
    "    'CO': (4, 8),\n",
    "    'Blood Flow Index': None,\n",
    "    'PPG waveform features': None,\n",
    "    'Signal Quality Index': (0, 100),\n",
    "    'Respiratory effort': None,\n",
    "    'O₂ extraction ratio': (0.2, 3),\n",
    "    'SNR': (20, 40)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09f2f6c-4e87-44e2-845b-fe707f9052d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnforceValueRanges(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ranges=None):\n",
    "        self.ranges = ranges\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.ranges is None:\n",
    "            return X\n",
    "\n",
    "        for col, valid_range in self.ranges.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            if valid_range is None:\n",
    "                continue  \n",
    "\n",
    "            low, high = valid_range\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "            mask = ~X[col].between(low, high, inclusive='both')\n",
    "            X.loc[mask, col] = np.nan\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b50c6-3799-48d4-ac2a-d08c9c0d5c2d",
   "metadata": {},
   "source": [
    "For table station we have to parse the location column to get continent and city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab7e746-79f1-4f34-9d5a-a37976760c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseLocation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column='location'):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.column not in X.columns:\n",
    "            return X\n",
    "        \n",
    "        split_cols = X[self.column].astype(str).str.split('/', n=1, expand=True)\n",
    "        X['continent'] = split_cols[0].str.strip()\n",
    "        X['city'] = split_cols[1].str.strip()\n",
    "        X = X.drop(columns=[self.column], errors='ignore')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482e4b1b-8bde-42eb-8f6d-005d23df1b31",
   "metadata": {},
   "source": [
    "Furthermore we create functions for other dataset transformations that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a02ac5c-7387-467a-908e-971d6b4ad0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns or []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X,  y=None):\n",
    "        if y is not None:\n",
    "            return X, y\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b8ce7d-7bb3-4f70-a67d-c917e7a00253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDuplicates(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X,  y=None):\n",
    "        if y is not None:\n",
    "            combined = pd.concat([X, y.reset_index(drop=True)], axis=1)\n",
    "            combined = combined.drop_duplicates()\n",
    "            X_transformed = combined[X.columns]\n",
    "            y_transformed = combined[y.name]\n",
    "            return X_transformed, y_transformed\n",
    "        else:\n",
    "            X_transformed = X.drop_duplicates()\n",
    "            return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f44b8e5-a292-4555-9bc6-3bbc87747dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNA(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, how='any', subset=None):\n",
    "        self.how = how\n",
    "        self.subset = subset\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X,  y=None):\n",
    "        self.mask_ = X.dropna(how=self.how, subset=self.subset).index\n",
    "        X_transformed = X.loc[self.mask_].copy()\n",
    "        \n",
    "        if y is not None:\n",
    "            y_transformed = y.loc[self.mask_].copy()\n",
    "            return X_transformed, y_transformed\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf746e1-2fad-43be-9c86-e20295df9f0e",
   "metadata": {},
   "source": [
    "Lastly we define first version of pipelines. \n",
    "For observation pipeline we first enforce schema and ranges so all the values are of correct type and in valid range. Then we remove coordinates since these are not valid medical information and we don't to use them for our model. Then we remove null values and remove duplicates.\n",
    "For station pipeline we first enforce schema to have all columns as correct data types, then we drop station name and revision date, because we dont want to use these coulumns and they wouldn't hold any relevant information. Then we parse the location to get continent and city of the station. Lastly we drop null values and remove duplicates. This dataset is now ready to provide us additional information about the obseravtions, if it would be needed we can remove deletion of coordinates columns and add to each observation innformation about continent, country (code) and city.\n",
    "Lastly for patient dataset we only enforce schema and remove duplicates (we don't remove null values because most of the columns are partially empty), this dataset does not provide any additional information for us as we found out in 1st phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b080dbc-e1cd-43a8-8b5f-c69b335ee41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_18860\\1183780406.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[col] = X[col].replace('nan', np.nan)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>revision</th>\n",
       "      <th>station</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>continent</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES</td>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>San Juan de Aznalfarache</td>\n",
       "      <td>37.35813</td>\n",
       "      <td>-6.03731</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Villa del Rosario</td>\n",
       "      <td>7.83389</td>\n",
       "      <td>-72.47417</td>\n",
       "      <td>America</td>\n",
       "      <td>Bogota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN</td>\n",
       "      <td>2022-09-04</td>\n",
       "      <td>Rangia</td>\n",
       "      <td>26.44931</td>\n",
       "      <td>91.61356</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Kolkata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>Rolla</td>\n",
       "      <td>37.95143</td>\n",
       "      <td>-91.77127</td>\n",
       "      <td>America</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>Albstadt</td>\n",
       "      <td>48.21644</td>\n",
       "      <td>9.02596</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code   revision                   station  latitude  longitude continent  \\\n",
       "0   ES 2016-12-22  San Juan de Aznalfarache  37.35813   -6.03731    Europe   \n",
       "1   CO 2023-07-22         Villa del Rosario   7.83389  -72.47417   America   \n",
       "2   IN 2022-09-04                    Rangia  26.44931   91.61356      Asia   \n",
       "3   US 2021-03-29                     Rolla  37.95143  -91.77127   America   \n",
       "4   DE 2023-09-12                  Albstadt  48.21644    9.02596    Europe   \n",
       "\n",
       "      city  \n",
       "0   Madrid  \n",
       "1   Bogota  \n",
       "2  Kolkata  \n",
       "3  Chicago  \n",
       "4   Berlin  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "patient_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=patient_schema)),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train,y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "transformed_patient = patient_pipeline.fit_transform(pd.read_csv(\"dataset/patient.csv\", sep='\\t'))\n",
    "\n",
    "transformed_station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae639950-ff60-4e42-8b56-ead60d49b0d5",
   "metadata": {},
   "source": [
    "### B - Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb66f96-d6da-4c4d-8eb3-573907f45926",
   "metadata": {},
   "source": [
    "Transform the data into a format suitable for machine learning, i.e. each observation must be described by one row, and each attribute must be numeric.\n",
    "Iteratively integrate preprocessing steps from Phase 1 as part of a unified process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc8fab-112d-45d9-9277-9bad261b9746",
   "metadata": {},
   "source": [
    "For observation dataset there are no string or category features, therefore we will show encoding of categorical features on station dataset. In pipeline we will still call encoding but it will have no effect. Furtermore we will only use station and observation datasets since we have relevant information there only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a764906-9ae6-42f2-b6f4-0e90ba24edc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat__continent_Africa</th>\n",
       "      <th>cat__continent_America</th>\n",
       "      <th>cat__continent_Asia</th>\n",
       "      <th>cat__continent_Atlantic</th>\n",
       "      <th>cat__continent_Australia</th>\n",
       "      <th>cat__continent_Europe</th>\n",
       "      <th>cat__continent_Indian</th>\n",
       "      <th>cat__continent_Pacific</th>\n",
       "      <th>cat__city_Abidjan</th>\n",
       "      <th>cat__city_Accra</th>\n",
       "      <th>...</th>\n",
       "      <th>cat__code_UY</th>\n",
       "      <th>cat__code_UZ</th>\n",
       "      <th>cat__code_VE</th>\n",
       "      <th>cat__code_VU</th>\n",
       "      <th>cat__code_YE</th>\n",
       "      <th>cat__code_ZA</th>\n",
       "      <th>remainder__revision</th>\n",
       "      <th>remainder__station</th>\n",
       "      <th>remainder__latitude</th>\n",
       "      <th>remainder__longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-12-22</td>\n",
       "      <td>San Juan de Aznalfarache</td>\n",
       "      <td>37.35813</td>\n",
       "      <td>-6.03731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Villa del Rosario</td>\n",
       "      <td>7.83389</td>\n",
       "      <td>-72.47417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-09-04</td>\n",
       "      <td>Rangia</td>\n",
       "      <td>26.44931</td>\n",
       "      <td>91.61356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-03-29</td>\n",
       "      <td>Rolla</td>\n",
       "      <td>37.95143</td>\n",
       "      <td>-91.77127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2023-09-12</td>\n",
       "      <td>Albstadt</td>\n",
       "      <td>48.21644</td>\n",
       "      <td>9.02596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat__continent_Africa  cat__continent_America  cat__continent_Asia  \\\n",
       "0                    0.0                     0.0                  0.0   \n",
       "1                    0.0                     1.0                  0.0   \n",
       "2                    0.0                     0.0                  1.0   \n",
       "3                    0.0                     1.0                  0.0   \n",
       "4                    0.0                     0.0                  0.0   \n",
       "\n",
       "   cat__continent_Atlantic  cat__continent_Australia  cat__continent_Europe  \\\n",
       "0                      0.0                       0.0                    1.0   \n",
       "1                      0.0                       0.0                    0.0   \n",
       "2                      0.0                       0.0                    0.0   \n",
       "3                      0.0                       0.0                    0.0   \n",
       "4                      0.0                       0.0                    1.0   \n",
       "\n",
       "   cat__continent_Indian  cat__continent_Pacific  cat__city_Abidjan  \\\n",
       "0                    0.0                     0.0                0.0   \n",
       "1                    0.0                     0.0                0.0   \n",
       "2                    0.0                     0.0                0.0   \n",
       "3                    0.0                     0.0                0.0   \n",
       "4                    0.0                     0.0                0.0   \n",
       "\n",
       "   cat__city_Accra  ...  cat__code_UY  cat__code_UZ  cat__code_VE  \\\n",
       "0              0.0  ...           0.0           0.0           0.0   \n",
       "1              0.0  ...           0.0           0.0           0.0   \n",
       "2              0.0  ...           0.0           0.0           0.0   \n",
       "3              0.0  ...           0.0           0.0           0.0   \n",
       "4              0.0  ...           0.0           0.0           0.0   \n",
       "\n",
       "   cat__code_VU  cat__code_YE  cat__code_ZA  remainder__revision  \\\n",
       "0           0.0           0.0           0.0           2016-12-22   \n",
       "1           0.0           0.0           0.0           2023-07-22   \n",
       "2           0.0           0.0           0.0           2022-09-04   \n",
       "3           0.0           0.0           0.0           2021-03-29   \n",
       "4           0.0           0.0           0.0           2023-09-12   \n",
       "\n",
       "         remainder__station  remainder__latitude  remainder__longitude  \n",
       "0  San Juan de Aznalfarache             37.35813              -6.03731  \n",
       "1         Villa del Rosario              7.83389             -72.47417  \n",
       "2                    Rangia             26.44931              91.61356  \n",
       "3                     Rolla             37.95143             -91.77127  \n",
       "4                  Albstadt             48.21644               9.02596  \n",
       "\n",
       "[5 rows x 240 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, [])\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code'])\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train,y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "\n",
    "transformed_station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a209fd-1e99-42a4-a77c-745d2cc75d00",
   "metadata": {},
   "source": [
    "For encoding we have put Column transformer in our pipelines, in here the transformations and scalers will be defined too. We have added remainder='passthrough' option in the column transformer to keep the columns that are not transformed in any way. After the transforms we had an issue with the step returning the numpy array which did not contain columns names, to fix this we have used .set_output(transform=\"pandas\") option for the column transformer. This will force the step to output pandas dataframe to the next step.\n",
    "For categorical columns we first replace null values with most frequent value and then we encode them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0307e7-086a-42b6-a078-9a01d1619b39",
   "metadata": {},
   "source": [
    "### C - Feature Scaling and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c29ca-ac4a-4bed-8aca-721c4916c769",
   "metadata": {},
   "source": [
    "Transform the dataset attributes for machine learning using at least the following techniques:\n",
    "\n",
    "- Scaling (2 techniques)\n",
    "- Transformers (2 techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b713aa3-5f8a-440d-b7c3-756f82ccd5bc",
   "metadata": {},
   "source": [
    " #### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c2f0b-abc0-4d8e-8496-0a62187392f6",
   "metadata": {},
   "source": [
    "##### StandardScaler\n",
    "For each numeric column, it computes formula: $z = (x - mean)/ std$, with mean = 0 and std = 1\n",
    "StandardScaler makes each column \"comparable\" in scale. Prevents some features being too influential simply because of their scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3638d6-e18f-4a15-a737-b0be71d8688c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__SpO₂</th>\n",
       "      <th>num__HR</th>\n",
       "      <th>num__PI</th>\n",
       "      <th>num__RR</th>\n",
       "      <th>num__EtCO₂</th>\n",
       "      <th>num__FiO₂</th>\n",
       "      <th>num__PRV</th>\n",
       "      <th>num__BP</th>\n",
       "      <th>num__Skin Temperature</th>\n",
       "      <th>num__Motion/Activity index</th>\n",
       "      <th>...</th>\n",
       "      <th>num__SV</th>\n",
       "      <th>num__CO</th>\n",
       "      <th>num__Blood Flow Index</th>\n",
       "      <th>num__PPG waveform features</th>\n",
       "      <th>num__Signal Quality Index</th>\n",
       "      <th>num__Respiratory effort</th>\n",
       "      <th>num__O₂ extraction ratio</th>\n",
       "      <th>num__SNR</th>\n",
       "      <th>num__latitude</th>\n",
       "      <th>num__longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.953852</td>\n",
       "      <td>0.032699</td>\n",
       "      <td>1.222765</td>\n",
       "      <td>0.732980</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>-0.229489</td>\n",
       "      <td>0.320303</td>\n",
       "      <td>1.900541</td>\n",
       "      <td>0.891793</td>\n",
       "      <td>1.388571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530179</td>\n",
       "      <td>-0.268116</td>\n",
       "      <td>-0.754105</td>\n",
       "      <td>-0.465557</td>\n",
       "      <td>-0.706253</td>\n",
       "      <td>1.279517</td>\n",
       "      <td>-0.539533</td>\n",
       "      <td>1.003425</td>\n",
       "      <td>-1.285032</td>\n",
       "      <td>0.279441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>-0.991307</td>\n",
       "      <td>0.266432</td>\n",
       "      <td>0.549494</td>\n",
       "      <td>-1.610561</td>\n",
       "      <td>-0.139343</td>\n",
       "      <td>1.056711</td>\n",
       "      <td>0.443323</td>\n",
       "      <td>0.585270</td>\n",
       "      <td>-0.307794</td>\n",
       "      <td>-0.968697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206366</td>\n",
       "      <td>-0.165222</td>\n",
       "      <td>0.820335</td>\n",
       "      <td>0.229577</td>\n",
       "      <td>-0.751795</td>\n",
       "      <td>0.979515</td>\n",
       "      <td>0.789101</td>\n",
       "      <td>-1.425010</td>\n",
       "      <td>-0.544045</td>\n",
       "      <td>0.898907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>-2.102740</td>\n",
       "      <td>0.593181</td>\n",
       "      <td>0.226362</td>\n",
       "      <td>-3.443886</td>\n",
       "      <td>-0.014848</td>\n",
       "      <td>1.979283</td>\n",
       "      <td>-0.563047</td>\n",
       "      <td>-0.775902</td>\n",
       "      <td>-0.102475</td>\n",
       "      <td>2.708076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.398170</td>\n",
       "      <td>0.088075</td>\n",
       "      <td>-0.844645</td>\n",
       "      <td>-0.020597</td>\n",
       "      <td>1.879169</td>\n",
       "      <td>1.608523</td>\n",
       "      <td>0.392014</td>\n",
       "      <td>-1.071256</td>\n",
       "      <td>-0.998730</td>\n",
       "      <td>1.252600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>-0.522682</td>\n",
       "      <td>-0.408683</td>\n",
       "      <td>-0.316020</td>\n",
       "      <td>-0.381465</td>\n",
       "      <td>0.273402</td>\n",
       "      <td>0.226677</td>\n",
       "      <td>-0.691574</td>\n",
       "      <td>-0.831748</td>\n",
       "      <td>-2.301060</td>\n",
       "      <td>-1.472041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810748</td>\n",
       "      <td>-0.429002</td>\n",
       "      <td>-1.756642</td>\n",
       "      <td>-0.921236</td>\n",
       "      <td>0.397168</td>\n",
       "      <td>-0.562005</td>\n",
       "      <td>1.130631</td>\n",
       "      <td>0.916572</td>\n",
       "      <td>-1.540835</td>\n",
       "      <td>-0.609127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10405</th>\n",
       "      <td>0.634981</td>\n",
       "      <td>0.591865</td>\n",
       "      <td>0.352991</td>\n",
       "      <td>0.862953</td>\n",
       "      <td>1.823320</td>\n",
       "      <td>-0.107193</td>\n",
       "      <td>0.331392</td>\n",
       "      <td>-1.149611</td>\n",
       "      <td>-1.598345</td>\n",
       "      <td>-0.775176</td>\n",
       "      <td>...</td>\n",
       "      <td>1.276494</td>\n",
       "      <td>0.048805</td>\n",
       "      <td>0.019867</td>\n",
       "      <td>0.402615</td>\n",
       "      <td>0.990483</td>\n",
       "      <td>0.075276</td>\n",
       "      <td>1.134043</td>\n",
       "      <td>1.293153</td>\n",
       "      <td>-0.095177</td>\n",
       "      <td>1.111728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num__SpO₂   num__HR   num__PI   num__RR  num__EtCO₂  num__FiO₂  \\\n",
       "108     1.953852  0.032699  1.222765  0.732980    0.382418  -0.229489   \n",
       "12092  -0.991307  0.266432  0.549494 -1.610561   -0.139343   1.056711   \n",
       "9658   -2.102740  0.593181  0.226362 -3.443886   -0.014848   1.979283   \n",
       "2929   -0.522682 -0.408683 -0.316020 -0.381465    0.273402   0.226677   \n",
       "10405   0.634981  0.591865  0.352991  0.862953    1.823320  -0.107193   \n",
       "\n",
       "       num__PRV   num__BP  num__Skin Temperature  num__Motion/Activity index  \\\n",
       "108    0.320303  1.900541               0.891793                    1.388571   \n",
       "12092  0.443323  0.585270              -0.307794                   -0.968697   \n",
       "9658  -0.563047 -0.775902              -0.102475                    2.708076   \n",
       "2929  -0.691574 -0.831748              -2.301060                   -1.472041   \n",
       "10405  0.331392 -1.149611              -1.598345                   -0.775176   \n",
       "\n",
       "       ...   num__SV   num__CO  num__Blood Flow Index  \\\n",
       "108    ...  0.530179 -0.268116              -0.754105   \n",
       "12092  ...  0.206366 -0.165222               0.820335   \n",
       "9658   ... -0.398170  0.088075              -0.844645   \n",
       "2929   ...  0.810748 -0.429002              -1.756642   \n",
       "10405  ...  1.276494  0.048805               0.019867   \n",
       "\n",
       "       num__PPG waveform features  num__Signal Quality Index  \\\n",
       "108                     -0.465557                  -0.706253   \n",
       "12092                    0.229577                  -0.751795   \n",
       "9658                    -0.020597                   1.879169   \n",
       "2929                    -0.921236                   0.397168   \n",
       "10405                    0.402615                   0.990483   \n",
       "\n",
       "       num__Respiratory effort  num__O₂ extraction ratio  num__SNR  \\\n",
       "108                   1.279517                 -0.539533  1.003425   \n",
       "12092                 0.979515                  0.789101 -1.425010   \n",
       "9658                  1.608523                  0.392014 -1.071256   \n",
       "2929                 -0.562005                  1.130631  0.916572   \n",
       "10405                 0.075276                  1.134043  1.293153   \n",
       "\n",
       "       num__latitude  num__longitude  \n",
       "108        -1.285032        0.279441  \n",
       "12092      -0.544045        0.898907  \n",
       "9658       -0.998730        1.252600  \n",
       "2929       -1.540835       -0.609127  \n",
       "10405      -0.095177        1.111728  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code']),\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train,y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "\n",
    "transformed_observation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84cbe10-2896-4806-849e-bbd5c0ad6d12",
   "metadata": {},
   "source": [
    "##### MinMax Scaler\n",
    "Maps the minimum of each feature to −1 and the maximum to 1. It uses following formula: $z = -1 + 2 * ((x - x(min) / (x(max) - x(min))$\n",
    "Preserves the original shape of the distribution, only makes the values more tightly bounded.\n",
    "Good when we need values from smaller ranges (here from -1 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "939b79b1-5e11-4a0a-9562-9249b2322b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__SpO₂</th>\n",
       "      <th>num__HR</th>\n",
       "      <th>num__PI</th>\n",
       "      <th>num__RR</th>\n",
       "      <th>num__EtCO₂</th>\n",
       "      <th>num__FiO₂</th>\n",
       "      <th>num__PRV</th>\n",
       "      <th>num__BP</th>\n",
       "      <th>num__Skin Temperature</th>\n",
       "      <th>num__Motion/Activity index</th>\n",
       "      <th>...</th>\n",
       "      <th>num__SV</th>\n",
       "      <th>num__CO</th>\n",
       "      <th>num__Blood Flow Index</th>\n",
       "      <th>num__PPG waveform features</th>\n",
       "      <th>num__Signal Quality Index</th>\n",
       "      <th>num__Respiratory effort</th>\n",
       "      <th>num__O₂ extraction ratio</th>\n",
       "      <th>num__SNR</th>\n",
       "      <th>num__latitude</th>\n",
       "      <th>num__longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.756534</td>\n",
       "      <td>-0.086035</td>\n",
       "      <td>0.385460</td>\n",
       "      <td>0.344547</td>\n",
       "      <td>0.046418</td>\n",
       "      <td>-0.025558</td>\n",
       "      <td>0.050369</td>\n",
       "      <td>0.542354</td>\n",
       "      <td>0.302071</td>\n",
       "      <td>0.275965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448987</td>\n",
       "      <td>-0.978535</td>\n",
       "      <td>-0.192627</td>\n",
       "      <td>-0.083837</td>\n",
       "      <td>-0.217594</td>\n",
       "      <td>0.468981</td>\n",
       "      <td>-0.319556</td>\n",
       "      <td>0.574116</td>\n",
       "      <td>-0.124499</td>\n",
       "      <td>0.150960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>-0.248824</td>\n",
       "      <td>-0.030191</td>\n",
       "      <td>0.143535</td>\n",
       "      <td>-0.409846</td>\n",
       "      <td>-0.080694</td>\n",
       "      <td>0.306877</td>\n",
       "      <td>0.083884</td>\n",
       "      <td>0.219843</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>-0.347967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392417</td>\n",
       "      <td>-0.973574</td>\n",
       "      <td>0.213642</td>\n",
       "      <td>0.081514</td>\n",
       "      <td>-0.229690</td>\n",
       "      <td>0.397657</td>\n",
       "      <td>0.448137</td>\n",
       "      <td>-0.825119</td>\n",
       "      <td>0.168459</td>\n",
       "      <td>0.431359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>-0.628222</td>\n",
       "      <td>0.047877</td>\n",
       "      <td>0.027425</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.050365</td>\n",
       "      <td>0.545327</td>\n",
       "      <td>-0.190286</td>\n",
       "      <td>-0.113924</td>\n",
       "      <td>0.060747</td>\n",
       "      <td>0.625218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286804</td>\n",
       "      <td>-0.961361</td>\n",
       "      <td>-0.215990</td>\n",
       "      <td>0.022005</td>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.547200</td>\n",
       "      <td>0.218698</td>\n",
       "      <td>-0.621290</td>\n",
       "      <td>-0.011306</td>\n",
       "      <td>0.591456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>-0.088854</td>\n",
       "      <td>-0.191492</td>\n",
       "      <td>-0.167469</td>\n",
       "      <td>-0.014196</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.092344</td>\n",
       "      <td>-0.225301</td>\n",
       "      <td>-0.127617</td>\n",
       "      <td>-0.472883</td>\n",
       "      <td>-0.481194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498002</td>\n",
       "      <td>-0.986293</td>\n",
       "      <td>-0.451321</td>\n",
       "      <td>-0.192228</td>\n",
       "      <td>0.075455</td>\n",
       "      <td>0.031171</td>\n",
       "      <td>0.645475</td>\n",
       "      <td>0.524072</td>\n",
       "      <td>-0.225633</td>\n",
       "      <td>-0.251246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10405</th>\n",
       "      <td>0.306325</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>0.072926</td>\n",
       "      <td>0.386386</td>\n",
       "      <td>0.397450</td>\n",
       "      <td>0.006051</td>\n",
       "      <td>0.053390</td>\n",
       "      <td>-0.205559</td>\n",
       "      <td>-0.302323</td>\n",
       "      <td>-0.296745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579367</td>\n",
       "      <td>-0.963255</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>0.122674</td>\n",
       "      <td>0.233029</td>\n",
       "      <td>0.182680</td>\n",
       "      <td>0.647446</td>\n",
       "      <td>0.741054</td>\n",
       "      <td>0.345925</td>\n",
       "      <td>0.527691</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num__SpO₂   num__HR   num__PI   num__RR  num__EtCO₂  num__FiO₂  \\\n",
       "108     0.756534 -0.086035  0.385460  0.344547    0.046418  -0.025558   \n",
       "12092  -0.248824 -0.030191  0.143535 -0.409846   -0.080694   0.306877   \n",
       "9658   -0.628222  0.047877  0.027425 -1.000000   -0.050365   0.545327   \n",
       "2929   -0.088854 -0.191492 -0.167469 -0.014196    0.019859   0.092344   \n",
       "10405   0.306325  0.047562  0.072926  0.386386    0.397450   0.006051   \n",
       "\n",
       "       num__PRV   num__BP  num__Skin Temperature  num__Motion/Activity index  \\\n",
       "108    0.050369  0.542354               0.302071                    0.275965   \n",
       "12092  0.083884  0.219843               0.010913                   -0.347967   \n",
       "9658  -0.190286 -0.113924               0.060747                    0.625218   \n",
       "2929  -0.225301 -0.127617              -0.472883                   -0.481194   \n",
       "10405  0.053390 -0.205559              -0.302323                   -0.296745   \n",
       "\n",
       "       ...   num__SV   num__CO  num__Blood Flow Index  \\\n",
       "108    ...  0.448987 -0.978535              -0.192627   \n",
       "12092  ...  0.392417 -0.973574               0.213642   \n",
       "9658   ...  0.286804 -0.961361              -0.215990   \n",
       "2929   ...  0.498002 -0.986293              -0.451321   \n",
       "10405  ...  0.579367 -0.963255               0.007089   \n",
       "\n",
       "       num__PPG waveform features  num__Signal Quality Index  \\\n",
       "108                     -0.083837                  -0.217594   \n",
       "12092                    0.081514                  -0.229690   \n",
       "9658                     0.022005                   0.469048   \n",
       "2929                    -0.192228                   0.075455   \n",
       "10405                    0.122674                   0.233029   \n",
       "\n",
       "       num__Respiratory effort  num__O₂ extraction ratio  num__SNR  \\\n",
       "108                   0.468981                 -0.319556  0.574116   \n",
       "12092                 0.397657                  0.448137 -0.825119   \n",
       "9658                  0.547200                  0.218698 -0.621290   \n",
       "2929                  0.031171                  0.645475  0.524072   \n",
       "10405                 0.182680                  0.647446  0.741054   \n",
       "\n",
       "       num__latitude  num__longitude  \n",
       "108        -0.124499        0.150960  \n",
       "12092       0.168459        0.431359  \n",
       "9658       -0.011306        0.591456  \n",
       "2929       -0.225633       -0.251246  \n",
       "10405       0.345925        0.527691  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    #(\"scaler\", StandardScaler()),\n",
    "    (\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code']),\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train,y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "\n",
    "transformed_observation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12649e6f-4ae8-40b2-be35-997ae6a330a0",
   "metadata": {},
   "source": [
    "We have put both scalers into our pipeline, but one will be commented out. In need we can use any of the scalers easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7d0d6-fea6-43ea-826d-95083b760724",
   "metadata": {},
   "source": [
    "#### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b130ef5-e851-4698-81e8-5b0d60e8d0bd",
   "metadata": {},
   "source": [
    "##### PowerTransformer (Yeo-Johnson method)\n",
    "\n",
    "Makes given data from skewed and non-normal into more Gaussian like shape.\n",
    "There is a big chance of model behaving better when the inputs we feed him are closer to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4c91c77-ef0e-462d-9d35-47161d9e5130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__SpO₂</th>\n",
       "      <th>num__HR</th>\n",
       "      <th>num__PI</th>\n",
       "      <th>num__RR</th>\n",
       "      <th>num__EtCO₂</th>\n",
       "      <th>num__FiO₂</th>\n",
       "      <th>num__PRV</th>\n",
       "      <th>num__BP</th>\n",
       "      <th>num__Skin Temperature</th>\n",
       "      <th>num__Motion/Activity index</th>\n",
       "      <th>...</th>\n",
       "      <th>num__SV</th>\n",
       "      <th>num__CO</th>\n",
       "      <th>num__Blood Flow Index</th>\n",
       "      <th>num__PPG waveform features</th>\n",
       "      <th>num__Signal Quality Index</th>\n",
       "      <th>num__Respiratory effort</th>\n",
       "      <th>num__O₂ extraction ratio</th>\n",
       "      <th>num__SNR</th>\n",
       "      <th>num__latitude</th>\n",
       "      <th>num__longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2.241529</td>\n",
       "      <td>0.035210</td>\n",
       "      <td>1.186962</td>\n",
       "      <td>0.715950</td>\n",
       "      <td>0.376513</td>\n",
       "      <td>-0.229246</td>\n",
       "      <td>0.321797</td>\n",
       "      <td>1.904921</td>\n",
       "      <td>0.892083</td>\n",
       "      <td>1.385230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524744</td>\n",
       "      <td>-0.225364</td>\n",
       "      <td>-0.755669</td>\n",
       "      <td>-0.466986</td>\n",
       "      <td>-0.704109</td>\n",
       "      <td>1.284423</td>\n",
       "      <td>-0.532905</td>\n",
       "      <td>1.001671</td>\n",
       "      <td>-1.453282</td>\n",
       "      <td>0.244643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>-1.009404</td>\n",
       "      <td>0.268736</td>\n",
       "      <td>0.608312</td>\n",
       "      <td>-1.543483</td>\n",
       "      <td>-0.146314</td>\n",
       "      <td>1.056657</td>\n",
       "      <td>0.444607</td>\n",
       "      <td>0.584205</td>\n",
       "      <td>-0.306466</td>\n",
       "      <td>-0.968297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198872</td>\n",
       "      <td>-0.039309</td>\n",
       "      <td>0.819897</td>\n",
       "      <td>0.228158</td>\n",
       "      <td>-0.749841</td>\n",
       "      <td>0.980227</td>\n",
       "      <td>0.792622</td>\n",
       "      <td>-1.450753</td>\n",
       "      <td>-0.762531</td>\n",
       "      <td>0.902748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>-1.890576</td>\n",
       "      <td>0.594747</td>\n",
       "      <td>0.309697</td>\n",
       "      <td>-2.918234</td>\n",
       "      <td>-0.021913</td>\n",
       "      <td>1.978583</td>\n",
       "      <td>-0.561582</td>\n",
       "      <td>-0.776612</td>\n",
       "      <td>-0.101027</td>\n",
       "      <td>2.687207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.405067</td>\n",
       "      <td>0.332801</td>\n",
       "      <td>-0.845873</td>\n",
       "      <td>-0.022195</td>\n",
       "      <td>1.871305</td>\n",
       "      <td>1.619050</td>\n",
       "      <td>0.399880</td>\n",
       "      <td>-1.073147</td>\n",
       "      <td>-1.229393</td>\n",
       "      <td>1.282138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>-0.588099</td>\n",
       "      <td>-0.406534</td>\n",
       "      <td>-0.229970</td>\n",
       "      <td>-0.431378</td>\n",
       "      <td>0.266957</td>\n",
       "      <td>0.226903</td>\n",
       "      <td>-0.690361</td>\n",
       "      <td>-0.832310</td>\n",
       "      <td>-2.307414</td>\n",
       "      <td>-1.476076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808427</td>\n",
       "      <td>-0.597474</td>\n",
       "      <td>-1.751408</td>\n",
       "      <td>-0.921782</td>\n",
       "      <td>0.399576</td>\n",
       "      <td>-0.567079</td>\n",
       "      <td>1.128137</td>\n",
       "      <td>0.918762</td>\n",
       "      <td>-1.588683</td>\n",
       "      <td>-0.631504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10405</th>\n",
       "      <td>0.597934</td>\n",
       "      <td>0.593434</td>\n",
       "      <td>0.428541</td>\n",
       "      <td>0.858602</td>\n",
       "      <td>1.840109</td>\n",
       "      <td>-0.106945</td>\n",
       "      <td>0.332869</td>\n",
       "      <td>-1.149125</td>\n",
       "      <td>-1.600628</td>\n",
       "      <td>-0.773598</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282045</td>\n",
       "      <td>0.332801</td>\n",
       "      <td>0.017320</td>\n",
       "      <td>0.401428</td>\n",
       "      <td>0.990062</td>\n",
       "      <td>0.069165</td>\n",
       "      <td>1.131478</td>\n",
       "      <td>1.276149</td>\n",
       "      <td>-0.247202</td>\n",
       "      <td>1.130796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num__SpO₂   num__HR   num__PI   num__RR  num__EtCO₂  num__FiO₂  \\\n",
       "108     2.241529  0.035210  1.186962  0.715950    0.376513  -0.229246   \n",
       "12092  -1.009404  0.268736  0.608312 -1.543483   -0.146314   1.056657   \n",
       "9658   -1.890576  0.594747  0.309697 -2.918234   -0.021913   1.978583   \n",
       "2929   -0.588099 -0.406534 -0.229970 -0.431378    0.266957   0.226903   \n",
       "10405   0.597934  0.593434  0.428541  0.858602    1.840109  -0.106945   \n",
       "\n",
       "       num__PRV   num__BP  num__Skin Temperature  num__Motion/Activity index  \\\n",
       "108    0.321797  1.904921               0.892083                    1.385230   \n",
       "12092  0.444607  0.584205              -0.306466                   -0.968297   \n",
       "9658  -0.561582 -0.776612              -0.101027                    2.687207   \n",
       "2929  -0.690361 -0.832310              -2.307414                   -1.476076   \n",
       "10405  0.332869 -1.149125              -1.600628                   -0.773598   \n",
       "\n",
       "       ...   num__SV   num__CO  num__Blood Flow Index  \\\n",
       "108    ...  0.524744 -0.225364              -0.755669   \n",
       "12092  ...  0.198872 -0.039309               0.819897   \n",
       "9658   ... -0.405067  0.332801              -0.845873   \n",
       "2929   ...  0.808427 -0.597474              -1.751408   \n",
       "10405  ...  1.282045  0.332801               0.017320   \n",
       "\n",
       "       num__PPG waveform features  num__Signal Quality Index  \\\n",
       "108                     -0.466986                  -0.704109   \n",
       "12092                    0.228158                  -0.749841   \n",
       "9658                    -0.022195                   1.871305   \n",
       "2929                    -0.921782                   0.399576   \n",
       "10405                    0.401428                   0.990062   \n",
       "\n",
       "       num__Respiratory effort  num__O₂ extraction ratio  num__SNR  \\\n",
       "108                   1.284423                 -0.532905  1.001671   \n",
       "12092                 0.980227                  0.792622 -1.450753   \n",
       "9658                  1.619050                  0.399880 -1.073147   \n",
       "2929                 -0.567079                  1.128137  0.918762   \n",
       "10405                 0.069165                  1.131478  1.276149   \n",
       "\n",
       "       num__latitude  num__longitude  \n",
       "108        -1.453282        0.244643  \n",
       "12092      -0.762531        0.902748  \n",
       "9658       -1.229393        1.282138  \n",
       "2929       -1.588683       -0.631504  \n",
       "10405      -0.247202        1.130796  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    #(\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code']),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train,y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "\n",
    "transformed_observation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71d989-5857-463e-ba30-b83f2f13caf3",
   "metadata": {},
   "source": [
    "##### Polynomial features\n",
    "\n",
    "We create new features which are results of us multiplying existing ones and by that expands the feature space.\n",
    "We are hoping that this will result in boosting accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f49dbbff-7126-4583-bfba-ac5ab9b42ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__SpO₂</th>\n",
       "      <th>num__HR</th>\n",
       "      <th>num__PI</th>\n",
       "      <th>num__RR</th>\n",
       "      <th>num__EtCO₂</th>\n",
       "      <th>num__FiO₂</th>\n",
       "      <th>num__PRV</th>\n",
       "      <th>num__BP</th>\n",
       "      <th>num__Skin Temperature</th>\n",
       "      <th>num__Motion/Activity index</th>\n",
       "      <th>...</th>\n",
       "      <th>num__O₂ extraction ratio^2</th>\n",
       "      <th>num__O₂ extraction ratio SNR</th>\n",
       "      <th>num__O₂ extraction ratio latitude</th>\n",
       "      <th>num__O₂ extraction ratio longitude</th>\n",
       "      <th>num__SNR^2</th>\n",
       "      <th>num__SNR latitude</th>\n",
       "      <th>num__SNR longitude</th>\n",
       "      <th>num__latitude^2</th>\n",
       "      <th>num__latitude longitude</th>\n",
       "      <th>num__longitude^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.953852</td>\n",
       "      <td>0.032699</td>\n",
       "      <td>1.222765</td>\n",
       "      <td>0.732980</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>-0.229489</td>\n",
       "      <td>0.320303</td>\n",
       "      <td>1.900541</td>\n",
       "      <td>0.891793</td>\n",
       "      <td>1.388571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.579083</td>\n",
       "      <td>0.527485</td>\n",
       "      <td>-1.259206</td>\n",
       "      <td>0.252091</td>\n",
       "      <td>1.000101</td>\n",
       "      <td>-1.245683</td>\n",
       "      <td>0.353315</td>\n",
       "      <td>-1.401214</td>\n",
       "      <td>-0.070430</td>\n",
       "      <td>-0.788591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>-0.991307</td>\n",
       "      <td>0.266432</td>\n",
       "      <td>0.549494</td>\n",
       "      <td>-1.610561</td>\n",
       "      <td>-0.139343</td>\n",
       "      <td>1.056711</td>\n",
       "      <td>0.443323</td>\n",
       "      <td>0.585270</td>\n",
       "      <td>-0.307794</td>\n",
       "      <td>-0.968697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765317</td>\n",
       "      <td>-0.922922</td>\n",
       "      <td>-0.475862</td>\n",
       "      <td>0.987467</td>\n",
       "      <td>-1.320641</td>\n",
       "      <td>-0.693880</td>\n",
       "      <td>0.604919</td>\n",
       "      <td>-1.155540</td>\n",
       "      <td>0.461603</td>\n",
       "      <td>0.046585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>-2.102740</td>\n",
       "      <td>0.593181</td>\n",
       "      <td>0.226362</td>\n",
       "      <td>-3.443886</td>\n",
       "      <td>-0.014848</td>\n",
       "      <td>1.979283</td>\n",
       "      <td>-0.563047</td>\n",
       "      <td>-0.775902</td>\n",
       "      <td>-0.102475</td>\n",
       "      <td>2.708076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342165</td>\n",
       "      <td>-0.755059</td>\n",
       "      <td>-0.973374</td>\n",
       "      <td>1.307817</td>\n",
       "      <td>-1.052870</td>\n",
       "      <td>-0.996725</td>\n",
       "      <td>0.949592</td>\n",
       "      <td>-1.381002</td>\n",
       "      <td>0.165893</td>\n",
       "      <td>0.844020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>-0.522682</td>\n",
       "      <td>-0.408683</td>\n",
       "      <td>-0.316020</td>\n",
       "      <td>-0.381465</td>\n",
       "      <td>0.273402</td>\n",
       "      <td>0.226677</td>\n",
       "      <td>-0.691574</td>\n",
       "      <td>-0.831748</td>\n",
       "      <td>-2.301060</td>\n",
       "      <td>-1.472041</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143825</td>\n",
       "      <td>1.468764</td>\n",
       "      <td>-1.559376</td>\n",
       "      <td>-0.667294</td>\n",
       "      <td>0.897621</td>\n",
       "      <td>-1.532609</td>\n",
       "      <td>-0.679436</td>\n",
       "      <td>-1.339685</td>\n",
       "      <td>0.073831</td>\n",
       "      <td>-0.738530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10405</th>\n",
       "      <td>0.634981</td>\n",
       "      <td>0.591865</td>\n",
       "      <td>0.352991</td>\n",
       "      <td>0.862953</td>\n",
       "      <td>1.823320</td>\n",
       "      <td>-0.107193</td>\n",
       "      <td>0.331392</td>\n",
       "      <td>-1.149611</td>\n",
       "      <td>-1.598345</td>\n",
       "      <td>-0.775176</td>\n",
       "      <td>...</td>\n",
       "      <td>1.147674</td>\n",
       "      <td>1.835078</td>\n",
       "      <td>0.049683</td>\n",
       "      <td>1.267830</td>\n",
       "      <td>1.352410</td>\n",
       "      <td>0.173206</td>\n",
       "      <td>1.396111</td>\n",
       "      <td>-0.700193</td>\n",
       "      <td>0.982122</td>\n",
       "      <td>0.498488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num__SpO₂   num__HR   num__PI   num__RR  num__EtCO₂  num__FiO₂  \\\n",
       "108     1.953852  0.032699  1.222765  0.732980    0.382418  -0.229489   \n",
       "12092  -0.991307  0.266432  0.549494 -1.610561   -0.139343   1.056711   \n",
       "9658   -2.102740  0.593181  0.226362 -3.443886   -0.014848   1.979283   \n",
       "2929   -0.522682 -0.408683 -0.316020 -0.381465    0.273402   0.226677   \n",
       "10405   0.634981  0.591865  0.352991  0.862953    1.823320  -0.107193   \n",
       "\n",
       "       num__PRV   num__BP  num__Skin Temperature  num__Motion/Activity index  \\\n",
       "108    0.320303  1.900541               0.891793                    1.388571   \n",
       "12092  0.443323  0.585270              -0.307794                   -0.968697   \n",
       "9658  -0.563047 -0.775902              -0.102475                    2.708076   \n",
       "2929  -0.691574 -0.831748              -2.301060                   -1.472041   \n",
       "10405  0.331392 -1.149611              -1.598345                   -0.775176   \n",
       "\n",
       "       ...  num__O₂ extraction ratio^2  num__O₂ extraction ratio SNR  \\\n",
       "108    ...                   -0.579083                      0.527485   \n",
       "12092  ...                    0.765317                     -0.922922   \n",
       "9658   ...                    0.342165                     -0.755059   \n",
       "2929   ...                    1.143825                      1.468764   \n",
       "10405  ...                    1.147674                      1.835078   \n",
       "\n",
       "       num__O₂ extraction ratio latitude  num__O₂ extraction ratio longitude  \\\n",
       "108                            -1.259206                            0.252091   \n",
       "12092                          -0.475862                            0.987467   \n",
       "9658                           -0.973374                            1.307817   \n",
       "2929                           -1.559376                           -0.667294   \n",
       "10405                           0.049683                            1.267830   \n",
       "\n",
       "       num__SNR^2  num__SNR latitude  num__SNR longitude  num__latitude^2  \\\n",
       "108      1.000101          -1.245683            0.353315        -1.401214   \n",
       "12092   -1.320641          -0.693880            0.604919        -1.155540   \n",
       "9658    -1.052870          -0.996725            0.949592        -1.381002   \n",
       "2929     0.897621          -1.532609           -0.679436        -1.339685   \n",
       "10405    1.352410           0.173206            1.396111        -0.700193   \n",
       "\n",
       "       num__latitude longitude  num__longitude^2  \n",
       "108                  -0.070430         -0.788591  \n",
       "12092                 0.461603          0.046585  \n",
       "9658                  0.165893          0.844020  \n",
       "2929                  0.073831         -0.738530  \n",
       "10405                 0.982122          0.498488  \n",
       "\n",
       "[5 rows x 275 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    #(\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code']),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train,y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "\n",
    "transformed_observation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b512-ae65-4923-b382-c2cba6c23ceb",
   "metadata": {},
   "source": [
    "### D - Justification and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d28cc5-25af-4025-b8ee-366d9461eb9c",
   "metadata": {},
   "source": [
    "We transformed the observation dataset to make it compatible with model training by ensuring it contains only numerical values. We also demonstrated two different methods for data transformation and scaling, which help the model learn more effectively. For each method we described how the process works and what this hopes to achieve. In 3rd phase we can use these methods to make our models more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9339e-5c24-414f-858d-500c944d687d",
   "metadata": {},
   "source": [
    "## 2.2 Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7379fb-af8d-4059-b891-aa7a5472f972",
   "metadata": {},
   "source": [
    "### A - Identification of Informative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee4daa-9b57-47d3-b04f-135ce3aaa006",
   "metadata": {},
   "source": [
    "Identify which attributes (features) in your data are informative with respect to the target variable (use at least 3 techniques and compare their results)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03caeea8-fa35-48ec-b560-d1ca48219a78",
   "metadata": {},
   "source": [
    "#### Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf79d4b-2a6c-4413-9063-2ba305430a0f",
   "metadata": {},
   "source": [
    "Removes features that have a low variance, we will use this technique before scaling, because scaling and transforming can change the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34edcd77-3674-41c5-815d-562a881182fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed features due to low variance:\n",
      "['CO', 'O₂ extraction ratio']\n",
      "Selected:\n",
      " ['SpO₂', 'HR', 'PI', 'RR', 'EtCO₂', 'FiO₂', 'PRV', 'BP', 'Skin Temperature', 'Motion/Activity index', 'PVI', 'Hb level', 'SV', 'Blood Flow Index', 'PPG waveform features', 'Signal Quality Index', 'Respiratory effort', 'SNR', 'latitude', 'longitude']\n"
     ]
    }
   ],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    #(\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"variance_threshold\", VarianceThreshold(threshold=0.01).set_output(transform=\"pandas\")),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code']),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train, y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "\n",
    "select = VarianceThreshold(threshold=0.01)\n",
    "select.fit(X_train)\n",
    "select_features = X_train.columns[select.get_support()]\n",
    "\n",
    "mask = select.get_support()\n",
    "\n",
    "removed_features = X_train.columns[~mask]\n",
    "\n",
    "print(\"Removed features due to low variance:\")\n",
    "print(removed_features.tolist())\n",
    "\n",
    "print(\"Selected:\\n\", select_features.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32811d41-6b19-4426-96b7-ba64eb7f6fb4",
   "metadata": {},
   "source": [
    "#### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ecde0-2e4f-4360-934e-da7722ab12a4",
   "metadata": {},
   "source": [
    "Measures the dependency between the variables, higher the values, higher the dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c30a245-706e-40ef-bbe5-f9edd38b97cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num__SpO₂' 'num__RR' 'num__EtCO₂' 'num__SpO₂^2' 'num__SpO₂ RR'\n",
      " 'num__SpO₂ EtCO₂' 'num__HR RR' 'num__RR^2' 'num__RR EtCO₂' 'num__RR BP'\n",
      " 'num__RR Skin Temperature' 'num__RR Motion/Activity index' 'num__RR PVI'\n",
      " 'num__RR Hb level' 'num__RR SV']\n"
     ]
    }
   ],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    #(\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"variance_threshold\", VarianceThreshold(threshold=0.01).set_output(transform=\"pandas\")),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "    (\"select_kbest\", SelectKBest(score_func=f_regression, k=15).set_output(transform=\"pandas\")) \n",
    "\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code']),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train, y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "\n",
    "select_kbest_step = observation_pipeline.named_steps['select_kbest']\n",
    "mask = select_kbest_step.get_support() \n",
    "column_transformer = observation_pipeline.named_steps['encode']\n",
    "feature_names = column_transformer.get_feature_names_out()\n",
    "selected_features = feature_names[mask]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dacfe4b-e951-4bdd-a754-b84b5f883628",
   "metadata": {},
   "source": [
    "#### Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2de2c6-1e44-47d3-a1a9-3f6fd2623a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    #(\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"variance_threshold\", VarianceThreshold(threshold=0.01).set_output(transform=\"pandas\")),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "    (\"select_kbest\", SelectKBest(score_func=f_regression, k=15).set_output(transform=\"pandas\")) \n",
    "\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code']),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transformed_observation = observation_pipeline.fit_transform(X_train, y_train)\n",
    "transformed_station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "\n",
    "select_kbest_step = observation_pipeline.named_steps['select_kbest']\n",
    "mask = select_kbest_step.get_support() \n",
    "column_transformer = observation_pipeline.named_steps['encode']\n",
    "feature_names = column_transformer.get_feature_names_out()\n",
    "selected_features = feature_names[mask]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa483780-560a-4e32-836b-05c3d640d417",
   "metadata": {},
   "source": [
    "### B - Ranking of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b331286-4b6f-4e33-b219-0ec53994f5a4",
   "metadata": {},
   "source": [
    "Rank the identified features by importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec93b5-049e-40cb-940c-671d53552f10",
   "metadata": {},
   "source": [
    "### C - Justification and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9de70-b409-45bf-a9e7-24e22afdfcdf",
   "metadata": {},
   "source": [
    "Justify your choices/decisions for implementation (i.e., provide documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff557b-b0d8-45dd-939b-f0c7e55a3eab",
   "metadata": {},
   "source": [
    "## 2.3 Reproducibility of Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190199a-1b7d-41fc-9d50-62918d367384",
   "metadata": {},
   "source": [
    "### Code Generalization for Reuse and Pipeline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7f3b3-fd34-4fe9-ae1b-d57d0a816d9c",
   "metadata": {},
   "source": [
    "Modify your preprocessing code for the training dataset so that it can be reused without further modifications to preprocess the test dataset in a machine learning context. Use the sklearn.pipeline functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
