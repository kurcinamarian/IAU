{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4110b7d-d5cd-4e78-b618-841eac6bf638",
   "metadata": {},
   "source": [
    "# Intelligent Data Analysis Project\n",
    "### Matej Bebej (50%), Marian Kurcina (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ef770-04f0-4a1b-8968-a093a397df5b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Assignment\n",
    "- Phase 2 - Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85d8a5-61a7-4bb5-ae86-c78db03585c6",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51d95b-e2d3-416b-bd59-4fb793c70703",
   "metadata": {},
   "source": [
    "Oxygen saturation is a key indicator of the proper functioning of the respiratory and circulatory systems. When its value drops to a critically low level, it may indicate life-threatening conditions such as hypoxemia, respiratory failure, or severe infections. In such cases, immediate intervention is essential. Traditional monitoring is performed using pulse oximeters, which, however, can be affected by noise, motion artifacts, or may have limitations in certain clinical situations.\n",
    "\n",
    "Modern machine learning–based approaches offer the possibility to estimate and predict critical oxygen saturation values with higher accuracy (critical oxygen saturation estimation). Models can utilize multimodal data, such as heart rate, respiratory rate, blood pressure, or sensor signals. By being trained on diverse datasets, it is possible to identify early warning signs of desaturation, filter out noise, and provide timely alerts even before oxygen saturation drops below a safe threshold.\n",
    "\n",
    "The goal of this assignment is to become familiar with the issue of oxygen saturation monitoring, understand the contribution of artificial intelligence, and design a solution that could improve critical care and reduce risks associated with undiagnosed hypoxemia.\n",
    "\n",
    "Each pair of students will work with an assigned dataset starting from Week 2. Your task is to predict the dependent variable “oximetry” (the predicted variable) using machine learning methods. In doing so, you will need to deal with various issues present in the data, such as inconsistent formats, missing values, outliers, and others.\n",
    "\n",
    "The expected outcomes of the project are:\n",
    "\n",
    "- the best-performing machine learning model, and\n",
    "\n",
    "- a data pipeline for building it from the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce20ec-579b-4e1f-bd06-44d5324f812e",
   "metadata": {},
   "source": [
    "# Phase 2 – Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe3e813-415c-4498-8873-635a77c81807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateparser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "observation = pd.read_csv(\"dataset/observation.csv\", sep='\\t')\n",
    "patient = pd.read_csv(\"dataset/patient.csv\", sep='\\t')\n",
    "station = pd.read_csv(\"dataset/station.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c9876-3f96-4afe-bbfe-d39cb93b99a2",
   "metadata": {},
   "source": [
    "In this phase, you are expected to carry out data preprocessing for machine learning. The result should be a dataset (CSV or TSV), where each observation is described by one row.\n",
    "Since scikit-learn only works with numerical data, something must be done with the non-numerical data.\n",
    "\n",
    "Ensure the preprocessing is reproducible on both the training and test datasets, so that you can repeat the process multiple times as needed (iteratively).\n",
    "\n",
    "Because preprocessing can change the shape and characteristics of the data, you may need to perform EDA (Exploratory Data Analysis) again as necessary. These techniques will not be graded again, but document any changes in the chosen methods.\n",
    "You can solve data-related issues iteratively across all phases, as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b823b-ecb7-467d-abf2-2af9d3700a6c",
   "metadata": {},
   "source": [
    "## 2.1 Implementation of Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bb233-dc76-4391-b893-beeb833cb867",
   "metadata": {},
   "source": [
    "### A - Train–Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db092e00-b43f-4e5d-b52a-c3d53baa35ab",
   "metadata": {},
   "source": [
    "Split the data into training and test sets according to your predefined ratio. Continue working only with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfbb841f-4181-4f0f-b03a-64305bcc5b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates: 1\n"
     ]
    }
   ],
   "source": [
    "before = len(observation)\n",
    "observation = observation.drop_duplicates()\n",
    "after = len(observation)\n",
    "print(\"Removed duplicates:\", before - after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc43d6c4-18be-4e11-bb8e-cae030f9a49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (9684, 23)\n",
      "test shape: (2422, 23)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(\n",
    "    observation, \n",
    "    test_size=0.2,        # 20% for testing\n",
    "    random_state=42,      # reproducibility\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"train shape:\", train.shape)\n",
    "print(\"test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef1b03-c01e-4000-91bf-32d9fab0d755",
   "metadata": {},
   "source": [
    "We split data into train and test parts, with 20% of observation data being test data () and 80% being train data ()."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e91b3d40-b7ea-4f1f-9d7a-4752bf3edaf0",
   "metadata": {},
   "source": [
    "First we preprocess train data by: \n",
    "- enforcing schemas on each data frame\n",
    "- removing latitude and longitude columns from observation since they are not medical data and have no corelation to oxymetry (as we learned in First Phase of the project - EDA)\n",
    "- removing logical outliers - values which are outside of acceptable range\n",
    "- removing records which have oxymetry missing \n",
    "- removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041761b8-351a-4dc7-8227-333ea88209b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_schema = {\n",
    "    'location':'string',\n",
    "    'code':'string',\n",
    "    'revision':'date',\n",
    "    'station':'string',\n",
    "    'latitude':'float',\n",
    "    'longitude':'float',\n",
    "}\n",
    "observation_schema = {\n",
    "    'SpO₂':'float',\n",
    "    'HR':'float',\n",
    "    'PI':'float',\n",
    "    'RR':'float',\n",
    "    'EtCO₂':'float',\n",
    "    'FiO₂':'float',\n",
    "    'PRV':'float',\n",
    "    'BP':'float',\n",
    "    'Skin Temperature':'float',\n",
    "    'Motion/Activity index':'float',\n",
    "    'PVI':'float',\n",
    "    'Hb level':'float',\n",
    "    'SV':'float',\n",
    "    'CO':'float',\n",
    "    'Blood Flow Index':'float',\n",
    "    'PPG waveform features':'float',\n",
    "    'Signal Quality Index':'float',\n",
    "    'Respiratory effort':'float',\n",
    "    'O₂ extraction ratio':'float',\n",
    "    'SNR':'float',\n",
    "    'oximetry':'int',\n",
    "    'latitude':'float',\n",
    "    'longitude':'float'\n",
    "}\n",
    "patient_schema = {\n",
    "    'residence':'string',              \n",
    "    'current_location':'string',    \n",
    "    'blood_group':'string',          \n",
    "    'job':'string',                 \n",
    "    'mail':'string',                \n",
    "    'user_id':'int',             \n",
    "    'birthdate':'date',           \n",
    "    'company':'string',             \n",
    "    'name':'string',                \n",
    "    'username':'string',            \n",
    "    'ssn':'string',                 \n",
    "    'registration':'date',        \n",
    "    'station_ID':'int'            \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a07760-5afc-43d2-9bf3-69ca14ac6b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnforceSchema(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, schema=None):\n",
    "        self.schema = schema\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.schema is None:\n",
    "            return X\n",
    "\n",
    "        X = X.copy()\n",
    "\n",
    "        for col, col_type in self.schema.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            if col_type == 'int':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce').astype('Int64')\n",
    "\n",
    "            elif col_type == 'float':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce').astype(float)\n",
    "\n",
    "            elif col_type == 'numeric':\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "            elif col_type == 'date':\n",
    "                X[col] = X[col].apply(\n",
    "                    lambda x: dateparser.parse(str(x))\n",
    "                    if pd.notnull(x) else pd.NaT\n",
    "                )\n",
    "\n",
    "            elif col_type == 'string':\n",
    "                X[col] = X[col].astype(str)\n",
    "                X[col] = X[col].replace('nan', np.nan)\n",
    "\n",
    "            elif col_type == 'bool':\n",
    "                X[col] = (\n",
    "                    X[col]\n",
    "                    .astype(str)\n",
    "                    .str.lower()\n",
    "                    .map({'true': True, 'false': False, '1': True, '0': False})\n",
    "                )\n",
    "\n",
    "            elif col_type == 'category':\n",
    "                X[col] = X[col].astype('category')\n",
    "\n",
    "    \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f957a35-c33d-41e2-bd14-d53f67dc7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ranges = {\n",
    "    'SpO₂': (95, 100),\n",
    "    'HR': (60, 100),\n",
    "    'PI': (0.2, 20),\n",
    "    'RR': (12, 20),\n",
    "    'EtCO₂': (35, 45),\n",
    "    'FiO₂': (21, 100),\n",
    "    'PRV': (20, 200),\n",
    "    'BP': (60, 120),\n",
    "    'Skin Temperature': (33, 38),\n",
    "    'Motion/Activity index': None,\n",
    "    'PVI': (10, 20),\n",
    "    'Hb level': (12, 18),\n",
    "    'SV': (60, 100),\n",
    "    'CO': (4, 8),\n",
    "    'Blood Flow Index': None,\n",
    "    'PPG waveform features': None,\n",
    "    'Signal Quality Index': (0, 100),\n",
    "    'Respiratory effort': None,\n",
    "    'O₂ extraction ratio': (0.2, 3),\n",
    "    'SNR': (20, 40)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e09f2f6c-4e87-44e2-845b-fe707f9052d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnforceValueRanges(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ranges=None):\n",
    "        self.ranges = ranges\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.ranges is None:\n",
    "            return X\n",
    "\n",
    "        for col, valid_range in self.ranges.items():\n",
    "            if col not in X.columns:\n",
    "                continue\n",
    "            if valid_range is None:\n",
    "                continue  \n",
    "\n",
    "            low, high = valid_range\n",
    "            X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "            mask = ~X[col].between(low, high, inclusive='both')\n",
    "            X.loc[mask, col] = np.nan\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab7e746-79f1-4f34-9d5a-a37976760c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseLocation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column='location'):\n",
    "        self.column = column\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if self.column not in X.columns:\n",
    "            return X\n",
    "        \n",
    "        split_cols = X[self.column].astype(str).str.split('/', n=1, expand=True)\n",
    "        X['continent'] = split_cols[0].str.strip()\n",
    "        X['city'] = split_cols[1].str.strip()\n",
    "        X = X.drop(columns=[self.column], errors='ignore')\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a02ac5c-7387-467a-908e-971d6b4ad0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns or []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop(columns=self.columns, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b8ce7d-7bb3-4f70-a67d-c917e7a00253",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveDuplicates(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f44b8e5-a292-4555-9bc6-3bbc87747dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropNA(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, how='any', subset=None):\n",
    "        self.how = how\n",
    "        self.subset = subset\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.dropna(how=self.how, subset=self.subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b080dbc-e1cd-43a8-8b5f-c69b335ee41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[col] = X[col].replace('nan', np.nan)\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>continent</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ES</td>\n",
       "      <td>37.35813</td>\n",
       "      <td>-6.03731</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO</td>\n",
       "      <td>7.83389</td>\n",
       "      <td>-72.47417</td>\n",
       "      <td>America</td>\n",
       "      <td>Bogota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IN</td>\n",
       "      <td>26.44931</td>\n",
       "      <td>91.61356</td>\n",
       "      <td>Asia</td>\n",
       "      <td>Kolkata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>37.95143</td>\n",
       "      <td>-91.77127</td>\n",
       "      <td>America</td>\n",
       "      <td>Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>48.21644</td>\n",
       "      <td>9.02596</td>\n",
       "      <td>Europe</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code  latitude  longitude continent     city\n",
       "0   ES  37.35813   -6.03731    Europe   Madrid\n",
       "1   CO   7.83389  -72.47417   America   Bogota\n",
       "2   IN  26.44931   91.61356      Asia  Kolkata\n",
       "3   US  37.95143  -91.77127   America  Chicago\n",
       "4   DE  48.21644    9.02596    Europe   Berlin"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "patient_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=patient_schema)),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "])\n",
    "\n",
    "observation = observation_pipeline.fit_transform(train)\n",
    "station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "patient = patient_pipeline.fit_transform(pd.read_csv(\"dataset/patient.csv\", sep='\\t'))\n",
    "\n",
    "station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae639950-ff60-4e42-8b56-ead60d49b0d5",
   "metadata": {},
   "source": [
    "### B - Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb66f96-d6da-4c4d-8eb3-573907f45926",
   "metadata": {},
   "source": [
    "Transform the data into a format suitable for machine learning, i.e. each observation must be described by one row, and each attribute must be numeric.\n",
    "Iteratively integrate preprocessing steps from Phase 1 as part of a unified process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc8fab-112d-45d9-9277-9bad261b9746",
   "metadata": {},
   "source": [
    "For observation dataset there are no string or category features, therefore we will show encoding on station dataset. In pipeline we will still call encoding but it will have no effect.\n",
    "\n",
    "We also ignore revision column and station column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a764906-9ae6-42f2-b6f4-0e90ba24edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[col] = X[col].replace('nan', np.nan)\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat__continent_Africa</th>\n",
       "      <th>cat__continent_America</th>\n",
       "      <th>cat__continent_Asia</th>\n",
       "      <th>cat__continent_Atlantic</th>\n",
       "      <th>cat__continent_Australia</th>\n",
       "      <th>cat__continent_Europe</th>\n",
       "      <th>cat__continent_Indian</th>\n",
       "      <th>cat__continent_Pacific</th>\n",
       "      <th>cat__city_Abidjan</th>\n",
       "      <th>cat__city_Accra</th>\n",
       "      <th>...</th>\n",
       "      <th>cat__code_UA</th>\n",
       "      <th>cat__code_US</th>\n",
       "      <th>cat__code_UY</th>\n",
       "      <th>cat__code_UZ</th>\n",
       "      <th>cat__code_VE</th>\n",
       "      <th>cat__code_VU</th>\n",
       "      <th>cat__code_YE</th>\n",
       "      <th>cat__code_ZA</th>\n",
       "      <th>remainder__latitude</th>\n",
       "      <th>remainder__longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.35813</td>\n",
       "      <td>-6.03731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.83389</td>\n",
       "      <td>-72.47417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.44931</td>\n",
       "      <td>91.61356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.95143</td>\n",
       "      <td>-91.77127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.21644</td>\n",
       "      <td>9.02596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 238 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cat__continent_Africa  cat__continent_America  cat__continent_Asia  \\\n",
       "0                    0.0                     0.0                  0.0   \n",
       "1                    0.0                     1.0                  0.0   \n",
       "2                    0.0                     0.0                  1.0   \n",
       "3                    0.0                     1.0                  0.0   \n",
       "4                    0.0                     0.0                  0.0   \n",
       "\n",
       "   cat__continent_Atlantic  cat__continent_Australia  cat__continent_Europe  \\\n",
       "0                      0.0                       0.0                    1.0   \n",
       "1                      0.0                       0.0                    0.0   \n",
       "2                      0.0                       0.0                    0.0   \n",
       "3                      0.0                       0.0                    0.0   \n",
       "4                      0.0                       0.0                    1.0   \n",
       "\n",
       "   cat__continent_Indian  cat__continent_Pacific  cat__city_Abidjan  \\\n",
       "0                    0.0                     0.0                0.0   \n",
       "1                    0.0                     0.0                0.0   \n",
       "2                    0.0                     0.0                0.0   \n",
       "3                    0.0                     0.0                0.0   \n",
       "4                    0.0                     0.0                0.0   \n",
       "\n",
       "   cat__city_Accra  ...  cat__code_UA  cat__code_US  cat__code_UY  \\\n",
       "0              0.0  ...           0.0           0.0           0.0   \n",
       "1              0.0  ...           0.0           0.0           0.0   \n",
       "2              0.0  ...           0.0           0.0           0.0   \n",
       "3              0.0  ...           0.0           1.0           0.0   \n",
       "4              0.0  ...           0.0           0.0           0.0   \n",
       "\n",
       "   cat__code_UZ  cat__code_VE  cat__code_VU  cat__code_YE  cat__code_ZA  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0   \n",
       "1           0.0           0.0           0.0           0.0           0.0   \n",
       "2           0.0           0.0           0.0           0.0           0.0   \n",
       "3           0.0           0.0           0.0           0.0           0.0   \n",
       "4           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "   remainder__latitude  remainder__longitude  \n",
       "0             37.35813              -6.03731  \n",
       "1              7.83389             -72.47417  \n",
       "2             26.44931              91.61356  \n",
       "3             37.95143             -91.77127  \n",
       "4             48.21644               9.02596  \n",
       "\n",
       "[5 rows x 238 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, [])\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['continent', 'city', 'code'])\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "patient_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=patient_schema)),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, ['blood_group'])\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "observation = observation_pipeline.fit_transform(train)\n",
    "station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "patient = patient_pipeline.fit_transform(pd.read_csv(\"dataset/patient.csv\", sep='\\t'))\n",
    "\n",
    "station.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f3638d6-e18f-4a15-a737-b0be71d8688c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[col] = X[col].replace('nan', np.nan)\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__SpO₂</th>\n",
       "      <th>num__HR</th>\n",
       "      <th>num__PI</th>\n",
       "      <th>num__RR</th>\n",
       "      <th>num__EtCO₂</th>\n",
       "      <th>num__FiO₂</th>\n",
       "      <th>num__PRV</th>\n",
       "      <th>num__BP</th>\n",
       "      <th>num__Skin Temperature</th>\n",
       "      <th>num__Motion/Activity index</th>\n",
       "      <th>...</th>\n",
       "      <th>num__Respiratory effort^2</th>\n",
       "      <th>num__Respiratory effort O₂ extraction ratio</th>\n",
       "      <th>num__Respiratory effort SNR</th>\n",
       "      <th>num__Respiratory effort oximetry</th>\n",
       "      <th>num__O₂ extraction ratio^2</th>\n",
       "      <th>num__O₂ extraction ratio SNR</th>\n",
       "      <th>num__O₂ extraction ratio oximetry</th>\n",
       "      <th>num__SNR^2</th>\n",
       "      <th>num__SNR oximetry</th>\n",
       "      <th>num__oximetry^2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1.954521</td>\n",
       "      <td>0.033698</td>\n",
       "      <td>1.224197</td>\n",
       "      <td>0.734080</td>\n",
       "      <td>0.380885</td>\n",
       "      <td>-0.228500</td>\n",
       "      <td>0.321029</td>\n",
       "      <td>1.904331</td>\n",
       "      <td>0.891956</td>\n",
       "      <td>1.386029</td>\n",
       "      <td>...</td>\n",
       "      <td>1.333800</td>\n",
       "      <td>0.770413</td>\n",
       "      <td>1.776819</td>\n",
       "      <td>1.280842</td>\n",
       "      <td>-0.578689</td>\n",
       "      <td>0.526920</td>\n",
       "      <td>0.677264</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>1.155678</td>\n",
       "      <td>0.813547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>-0.992378</td>\n",
       "      <td>0.267028</td>\n",
       "      <td>0.550853</td>\n",
       "      <td>-1.609397</td>\n",
       "      <td>-0.140488</td>\n",
       "      <td>1.056885</td>\n",
       "      <td>0.444067</td>\n",
       "      <td>0.587254</td>\n",
       "      <td>-0.308649</td>\n",
       "      <td>-0.970241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.967185</td>\n",
       "      <td>1.308606</td>\n",
       "      <td>-0.455511</td>\n",
       "      <td>-1.170694</td>\n",
       "      <td>0.763855</td>\n",
       "      <td>-0.923175</td>\n",
       "      <td>-1.209004</td>\n",
       "      <td>-1.321273</td>\n",
       "      <td>-1.175528</td>\n",
       "      <td>-1.229185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>-2.104468</td>\n",
       "      <td>0.593214</td>\n",
       "      <td>0.227687</td>\n",
       "      <td>-3.442672</td>\n",
       "      <td>-0.016086</td>\n",
       "      <td>1.978872</td>\n",
       "      <td>-0.562443</td>\n",
       "      <td>-0.775785</td>\n",
       "      <td>-0.103155</td>\n",
       "      <td>2.704977</td>\n",
       "      <td>...</td>\n",
       "      <td>1.756834</td>\n",
       "      <td>1.643355</td>\n",
       "      <td>0.191396</td>\n",
       "      <td>-1.170694</td>\n",
       "      <td>0.341288</td>\n",
       "      <td>-0.755348</td>\n",
       "      <td>-1.209004</td>\n",
       "      <td>-1.053444</td>\n",
       "      <td>-1.175528</td>\n",
       "      <td>-1.229185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>-0.523476</td>\n",
       "      <td>-0.406923</td>\n",
       "      <td>-0.314754</td>\n",
       "      <td>-0.380334</td>\n",
       "      <td>0.271949</td>\n",
       "      <td>0.227378</td>\n",
       "      <td>-0.690989</td>\n",
       "      <td>-0.831707</td>\n",
       "      <td>-2.303606</td>\n",
       "      <td>-1.473373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.628914</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>0.144519</td>\n",
       "      <td>-1.170694</td>\n",
       "      <td>1.141841</td>\n",
       "      <td>1.467996</td>\n",
       "      <td>-1.209004</td>\n",
       "      <td>0.897467</td>\n",
       "      <td>-1.175528</td>\n",
       "      <td>-1.229185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10405</th>\n",
       "      <td>0.634871</td>\n",
       "      <td>0.591900</td>\n",
       "      <td>0.354329</td>\n",
       "      <td>0.864050</td>\n",
       "      <td>1.820713</td>\n",
       "      <td>-0.106281</td>\n",
       "      <td>0.332120</td>\n",
       "      <td>-1.150007</td>\n",
       "      <td>-1.600295</td>\n",
       "      <td>-0.776803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027473</td>\n",
       "      <td>0.627438</td>\n",
       "      <td>0.942852</td>\n",
       "      <td>0.803044</td>\n",
       "      <td>1.145685</td>\n",
       "      <td>1.834231</td>\n",
       "      <td>1.066976</td>\n",
       "      <td>1.352354</td>\n",
       "      <td>1.264562</td>\n",
       "      <td>0.813547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num__SpO₂   num__HR   num__PI   num__RR  num__EtCO₂  num__FiO₂  \\\n",
       "108     1.954521  0.033698  1.224197  0.734080    0.380885  -0.228500   \n",
       "12092  -0.992378  0.267028  0.550853 -1.609397   -0.140488   1.056885   \n",
       "9658   -2.104468  0.593214  0.227687 -3.442672   -0.016086   1.978872   \n",
       "2929   -0.523476 -0.406923 -0.314754 -0.380334    0.271949   0.227378   \n",
       "10405   0.634871  0.591900  0.354329  0.864050    1.820713  -0.106281   \n",
       "\n",
       "       num__PRV   num__BP  num__Skin Temperature  num__Motion/Activity index  \\\n",
       "108    0.321029  1.904331               0.891956                    1.386029   \n",
       "12092  0.444067  0.587254              -0.308649                   -0.970241   \n",
       "9658  -0.562443 -0.775785              -0.103155                    2.704977   \n",
       "2929  -0.690989 -0.831707              -2.303606                   -1.473373   \n",
       "10405  0.332120 -1.150007              -1.600295                   -0.776803   \n",
       "\n",
       "       ...  num__Respiratory effort^2  \\\n",
       "108    ...                   1.333800   \n",
       "12092  ...                   0.967185   \n",
       "9658   ...                   1.756834   \n",
       "2929   ...                  -0.628914   \n",
       "10405  ...                  -0.027473   \n",
       "\n",
       "       num__Respiratory effort O₂ extraction ratio  \\\n",
       "108                                       0.770413   \n",
       "12092                                     1.308606   \n",
       "9658                                      1.643355   \n",
       "2929                                      0.002941   \n",
       "10405                                     0.627438   \n",
       "\n",
       "       num__Respiratory effort SNR  num__Respiratory effort oximetry  \\\n",
       "108                       1.776819                          1.280842   \n",
       "12092                    -0.455511                         -1.170694   \n",
       "9658                      0.191396                         -1.170694   \n",
       "2929                      0.144519                         -1.170694   \n",
       "10405                     0.942852                          0.803044   \n",
       "\n",
       "       num__O₂ extraction ratio^2  num__O₂ extraction ratio SNR  \\\n",
       "108                     -0.578689                      0.526920   \n",
       "12092                    0.763855                     -0.923175   \n",
       "9658                     0.341288                     -0.755348   \n",
       "2929                     1.141841                      1.467996   \n",
       "10405                    1.145685                      1.834231   \n",
       "\n",
       "       num__O₂ extraction ratio oximetry  num__SNR^2  num__SNR oximetry  \\\n",
       "108                             0.677264    0.999970           1.155678   \n",
       "12092                          -1.209004   -1.321273          -1.175528   \n",
       "9658                           -1.209004   -1.053444          -1.175528   \n",
       "2929                           -1.209004    0.897467          -1.175528   \n",
       "10405                           1.066976    1.352354           1.264562   \n",
       "\n",
       "       num__oximetry^2  \n",
       "108           0.813547  \n",
       "12092        -1.229185  \n",
       "9658         -1.229185  \n",
       "2929         -1.229185  \n",
       "10405         0.813547  \n",
       "\n",
       "[5 rows x 252 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    #(\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "patient_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=patient_schema)),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, [])\n",
    "        #('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "observation = observation_pipeline.fit_transform(train)\n",
    "station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "patient = patient_pipeline.fit_transform(pd.read_csv(\"dataset/patient.csv\", sep='\\t'))\n",
    "\n",
    "observation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c2f0b-abc0-4d8e-8496-0a62187392f6",
   "metadata": {},
   "source": [
    "Logic behind StandardScaler\n",
    "\n",
    "For each numeric column, it computes formula: z = (x - mean)/ std.\n",
    "\n",
    "mean = 0\n",
    "\n",
    "std = 1\n",
    "\n",
    "StandardScaler makes each column \"comparable\" in scale. Prevents some features being too influential simply because of their scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84cbe10-2896-4806-849e-bbd5c0ad6d12",
   "metadata": {},
   "source": [
    "Logic behind MinMax Scaler\n",
    "\n",
    "Maps the minimum of each feature to −1 and the maximum to 1. It uses following formula z = -1 + 2 * ((x - x(min) / (x(max) - x(min)).\n",
    "\n",
    "Preserves the original shape of the distribution, only makes the values more tightly bounded.\n",
    "\n",
    "Good when we need values from from smaller ranges (here from -1 to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4c91c77-ef0e-462d-9d35-47161d9e5130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X[col] = X[col].replace('nan', np.nan)\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n",
      "C:\\Users\\mbebe\\AppData\\Local\\Temp\\ipykernel_27176\\1183780406.py:28: DeprecationWarning: Parsing dates involving a day of month without a year specified is ambiguious\n",
      "and fails to parse leap day. The default behavior will change in Python 3.15\n",
      "to either always raise an exception or to use a different default year (TBD).\n",
      "To avoid trouble, add a specific year to the input & format.\n",
      "See https://github.com/python/cpython/issues/70647.\n",
      "  lambda x: dateparser.parse(str(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num__SpO₂</th>\n",
       "      <th>num__HR</th>\n",
       "      <th>num__PI</th>\n",
       "      <th>num__RR</th>\n",
       "      <th>num__EtCO₂</th>\n",
       "      <th>num__FiO₂</th>\n",
       "      <th>num__PRV</th>\n",
       "      <th>num__BP</th>\n",
       "      <th>num__Skin Temperature</th>\n",
       "      <th>num__Motion/Activity index</th>\n",
       "      <th>...</th>\n",
       "      <th>num__Hb level</th>\n",
       "      <th>num__SV</th>\n",
       "      <th>num__CO</th>\n",
       "      <th>num__Blood Flow Index</th>\n",
       "      <th>num__PPG waveform features</th>\n",
       "      <th>num__Signal Quality Index</th>\n",
       "      <th>num__Respiratory effort</th>\n",
       "      <th>num__O₂ extraction ratio</th>\n",
       "      <th>num__SNR</th>\n",
       "      <th>num__oximetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2.244423</td>\n",
       "      <td>0.036336</td>\n",
       "      <td>1.188052</td>\n",
       "      <td>0.717171</td>\n",
       "      <td>0.375025</td>\n",
       "      <td>-0.227016</td>\n",
       "      <td>0.322453</td>\n",
       "      <td>1.906521</td>\n",
       "      <td>0.892475</td>\n",
       "      <td>1.382773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.873398</td>\n",
       "      <td>0.524507</td>\n",
       "      <td>-0.312674</td>\n",
       "      <td>-0.754727</td>\n",
       "      <td>-0.463949</td>\n",
       "      <td>-0.705653</td>\n",
       "      <td>1.282818</td>\n",
       "      <td>-0.532311</td>\n",
       "      <td>1.001464</td>\n",
       "      <td>0.813547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12092</th>\n",
       "      <td>-1.010517</td>\n",
       "      <td>0.269449</td>\n",
       "      <td>0.610112</td>\n",
       "      <td>-1.542601</td>\n",
       "      <td>-0.147392</td>\n",
       "      <td>1.056563</td>\n",
       "      <td>0.445290</td>\n",
       "      <td>0.586723</td>\n",
       "      <td>-0.306314</td>\n",
       "      <td>-0.969859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297935</td>\n",
       "      <td>0.198690</td>\n",
       "      <td>-0.029034</td>\n",
       "      <td>0.820387</td>\n",
       "      <td>0.230167</td>\n",
       "      <td>-0.751392</td>\n",
       "      <td>0.978776</td>\n",
       "      <td>0.791416</td>\n",
       "      <td>-1.450768</td>\n",
       "      <td>-1.229185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>-1.890977</td>\n",
       "      <td>0.594859</td>\n",
       "      <td>0.311683</td>\n",
       "      <td>-2.918267</td>\n",
       "      <td>-0.023086</td>\n",
       "      <td>1.974625</td>\n",
       "      <td>-0.561045</td>\n",
       "      <td>-0.776138</td>\n",
       "      <td>-0.100607</td>\n",
       "      <td>2.684545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.373125</td>\n",
       "      <td>-0.405019</td>\n",
       "      <td>0.254606</td>\n",
       "      <td>-0.844819</td>\n",
       "      <td>-0.019760</td>\n",
       "      <td>1.871517</td>\n",
       "      <td>1.617233</td>\n",
       "      <td>0.399270</td>\n",
       "      <td>-1.073636</td>\n",
       "      <td>-1.229185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>-0.589237</td>\n",
       "      <td>-0.404662</td>\n",
       "      <td>-0.227979</td>\n",
       "      <td>-0.430180</td>\n",
       "      <td>0.265558</td>\n",
       "      <td>0.228752</td>\n",
       "      <td>-0.689830</td>\n",
       "      <td>-0.831986</td>\n",
       "      <td>-2.314869</td>\n",
       "      <td>-1.477345</td>\n",
       "      <td>...</td>\n",
       "      <td>1.728421</td>\n",
       "      <td>0.808179</td>\n",
       "      <td>-0.596314</td>\n",
       "      <td>-1.748582</td>\n",
       "      <td>-0.918355</td>\n",
       "      <td>0.398479</td>\n",
       "      <td>-0.568385</td>\n",
       "      <td>1.126381</td>\n",
       "      <td>0.918460</td>\n",
       "      <td>-1.229185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10405</th>\n",
       "      <td>0.597670</td>\n",
       "      <td>0.593550</td>\n",
       "      <td>0.430469</td>\n",
       "      <td>0.859807</td>\n",
       "      <td>1.837275</td>\n",
       "      <td>-0.104766</td>\n",
       "      <td>0.333528</td>\n",
       "      <td>-1.149760</td>\n",
       "      <td>-1.604353</td>\n",
       "      <td>-0.775262</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.126080</td>\n",
       "      <td>1.281858</td>\n",
       "      <td>0.254606</td>\n",
       "      <td>0.017713</td>\n",
       "      <td>0.403107</td>\n",
       "      <td>0.989404</td>\n",
       "      <td>0.067939</td>\n",
       "      <td>1.129716</td>\n",
       "      <td>1.276301</td>\n",
       "      <td>0.813547</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       num__SpO₂   num__HR   num__PI   num__RR  num__EtCO₂  num__FiO₂  \\\n",
       "108     2.244423  0.036336  1.188052  0.717171    0.375025  -0.227016   \n",
       "12092  -1.010517  0.269449  0.610112 -1.542601   -0.147392   1.056563   \n",
       "9658   -1.890977  0.594859  0.311683 -2.918267   -0.023086   1.974625   \n",
       "2929   -0.589237 -0.404662 -0.227979 -0.430180    0.265558   0.228752   \n",
       "10405   0.597670  0.593550  0.430469  0.859807    1.837275  -0.104766   \n",
       "\n",
       "       num__PRV   num__BP  num__Skin Temperature  num__Motion/Activity index  \\\n",
       "108    0.322453  1.906521               0.892475                    1.382773   \n",
       "12092  0.445290  0.586723              -0.306314                   -0.969859   \n",
       "9658  -0.561045 -0.776138              -0.100607                    2.684545   \n",
       "2929  -0.689830 -0.831986              -2.314869                   -1.477345   \n",
       "10405  0.333528 -1.149760              -1.604353                   -0.775262   \n",
       "\n",
       "       ...  num__Hb level   num__SV   num__CO  num__Blood Flow Index  \\\n",
       "108    ...      -0.873398  0.524507 -0.312674              -0.754727   \n",
       "12092  ...      -0.297935  0.198690 -0.029034               0.820387   \n",
       "9658   ...       0.373125 -0.405019  0.254606              -0.844819   \n",
       "2929   ...       1.728421  0.808179 -0.596314              -1.748582   \n",
       "10405  ...      -2.126080  1.281858  0.254606               0.017713   \n",
       "\n",
       "       num__PPG waveform features  num__Signal Quality Index  \\\n",
       "108                     -0.463949                  -0.705653   \n",
       "12092                    0.230167                  -0.751392   \n",
       "9658                    -0.019760                   1.871517   \n",
       "2929                    -0.918355                   0.398479   \n",
       "10405                    0.403107                   0.989404   \n",
       "\n",
       "       num__Respiratory effort  num__O₂ extraction ratio  num__SNR  \\\n",
       "108                   1.282818                 -0.532311  1.001464   \n",
       "12092                 0.978776                  0.791416 -1.450768   \n",
       "9658                  1.617233                  0.399270 -1.073636   \n",
       "2929                 -0.568385                  1.126381  0.918460   \n",
       "10405                 0.067939                  1.129716  1.276301   \n",
       "\n",
       "       num__oximetry  \n",
       "108         0.813547  \n",
       "12092      -1.229185  \n",
       "9658       -1.229185  \n",
       "2929       -1.229185  \n",
       "10405       0.813547  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_numeric_columns(df):\n",
    "    return df.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    #(\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    #(\"scaler\", MinMaxScaler(feature_range=(-1, 1)))\n",
    "])\n",
    "\n",
    "observation_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=observation_schema)),\n",
    "    (\"ranges\", EnforceValueRanges(ranges=valid_ranges)),\n",
    "    (\"drop_geo\", DropColumns(columns=['latitude', 'longitude'])),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),  \n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "station_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=station_schema)),\n",
    "    (\"drop_station_and_date\", DropColumns(columns=['station', 'revision'])),\n",
    "    (\"parse_location\", ParseLocation(column='location')),\n",
    "    (\"drop_na\", DropNA(how='any')),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, []),\n",
    "        ('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "patient_pipeline = Pipeline([\n",
    "    (\"schema\", EnforceSchema(schema=patient_schema)),\n",
    "    (\"remove_duplicates\", RemoveDuplicates()),\n",
    "    (\"encode\", ColumnTransformer([\n",
    "        ('cat', cat_pipeline, [])\n",
    "        #('num', numeric_pipeline, get_numeric_columns)\n",
    "    ], remainder='passthrough').set_output(transform=\"pandas\")),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "observation = observation_pipeline.fit_transform(train)\n",
    "station = station_pipeline.fit_transform(pd.read_csv(\"dataset/station.csv\", sep='\\t'))\n",
    "patient = patient_pipeline.fit_transform(pd.read_csv(\"dataset/patient.csv\", sep='\\t'))\n",
    "\n",
    "observation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b130ef5-e851-4698-81e8-5b0d60e8d0bd",
   "metadata": {},
   "source": [
    "PowerTransformer (Yeo-Johnson methjod)\n",
    "\n",
    "Makes given data from skewed and non-normal into more Gaussian like shape.\n",
    "\n",
    "There is a big chance of model behaving better when the inputs we feed him are closer to normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71d989-5857-463e-ba30-b83f2f13caf3",
   "metadata": {},
   "source": [
    "Polynomial features\n",
    "\n",
    "We create new features which are results of us multiplying existing ones and by that expands the feature space.\n",
    "\n",
    "We are hoping that this will result in boosting accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0307e7-086a-42b6-a078-9a01d1619b39",
   "metadata": {},
   "source": [
    "### C - Feature Scaling and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c87a4e-b084-4604-b513-56ba35f34b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c26c29ca-ac4a-4bed-8aca-721c4916c769",
   "metadata": {},
   "source": [
    "Transform the dataset attributes for machine learning using at least the following techniques:\n",
    "\n",
    "- Scaling (2 techniques)\n",
    "- Transformers (2 techniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b512-ae65-4923-b382-c2cba6c23ceb",
   "metadata": {},
   "source": [
    "### D - Justification and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d28cc5-25af-4025-b8ee-366d9461eb9c",
   "metadata": {},
   "source": [
    "Justify your choices/decisions for implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9339e-5c24-414f-858d-500c944d687d",
   "metadata": {},
   "source": [
    "## 2.2 Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7379fb-af8d-4059-b891-aa7a5472f972",
   "metadata": {},
   "source": [
    "### A - Identification of Informative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee4daa-9b57-47d3-b04f-135ce3aaa006",
   "metadata": {},
   "source": [
    "Identify which attributes (features) in your data are informative with respect to the target variable (use at least 3 techniques and compare their results)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa483780-560a-4e32-836b-05c3d640d417",
   "metadata": {},
   "source": [
    "### B - Ranking of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b331286-4b6f-4e33-b219-0ec53994f5a4",
   "metadata": {},
   "source": [
    "Rank the identified features by importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec93b5-049e-40cb-940c-671d53552f10",
   "metadata": {},
   "source": [
    "### C - Justification and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9de70-b409-45bf-a9e7-24e22afdfcdf",
   "metadata": {},
   "source": [
    "Justify your choices/decisions for implementation (i.e., provide documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff557b-b0d8-45dd-939b-f0c7e55a3eab",
   "metadata": {},
   "source": [
    "## 2.3 Reproducibility of Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190199a-1b7d-41fc-9d50-62918d367384",
   "metadata": {},
   "source": [
    "### Code Generalization for Reuse and Pipeline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e7f3b3-fd34-4fe9-ae1b-d57d0a816d9c",
   "metadata": {},
   "source": [
    "Modify your preprocessing code for the training dataset so that it can be reused without further modifications to preprocess the test dataset in a machine learning context. Use the sklearn.pipeline functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
