{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4110b7d-d5cd-4e78-b618-841eac6bf638",
   "metadata": {},
   "source": [
    "# Intelligent Data Analysis Project\n",
    "### Matej Bebej (50%), Marian Kurcina (50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ef770-04f0-4a1b-8968-a093a397df5b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- Assignment\n",
    "- Phase 2 - Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85d8a5-61a7-4bb5-ae86-c78db03585c6",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51d95b-e2d3-416b-bd59-4fb793c70703",
   "metadata": {},
   "source": [
    "Oxygen saturation is a key indicator of the proper functioning of the respiratory and circulatory systems. When its value drops to a critically low level, it may indicate life-threatening conditions such as hypoxemia, respiratory failure, or severe infections. In such cases, immediate intervention is essential. Traditional monitoring is performed using pulse oximeters, which, however, can be affected by noise, motion artifacts, or may have limitations in certain clinical situations.\n",
    "\n",
    "Modern machine learning–based approaches offer the possibility to estimate and predict critical oxygen saturation values with higher accuracy (critical oxygen saturation estimation). Models can utilize multimodal data, such as heart rate, respiratory rate, blood pressure, or sensor signals. By being trained on diverse datasets, it is possible to identify early warning signs of desaturation, filter out noise, and provide timely alerts even before oxygen saturation drops below a safe threshold.\n",
    "\n",
    "The goal of this assignment is to become familiar with the issue of oxygen saturation monitoring, understand the contribution of artificial intelligence, and design a solution that could improve critical care and reduce risks associated with undiagnosed hypoxemia.\n",
    "\n",
    "Each pair of students will work with an assigned dataset starting from Week 2. Your task is to predict the dependent variable “oximetry” (the predicted variable) using machine learning methods. In doing so, you will need to deal with various issues present in the data, such as inconsistent formats, missing values, outliers, and others.\n",
    "\n",
    "The expected outcomes of the project are:\n",
    "\n",
    "- the best-performing machine learning model, and\n",
    "\n",
    "- a data pipeline for building it from the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce20ec-579b-4e1f-bd06-44d5324f812e",
   "metadata": {},
   "source": [
    "# Phase 2 – Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fe3e813-415c-4498-8873-635a77c81807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateparser\n",
    "observation = pd.read_csv(\"dataset/observation.csv\", sep='\\t')\n",
    "patient = pd.read_csv(\"dataset/patient.csv\", sep='\\t')\n",
    "station = pd.read_csv(\"dataset/station.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c9876-3f96-4afe-bbfe-d39cb93b99a2",
   "metadata": {},
   "source": [
    "In this phase, you are expected to carry out data preprocessing for machine learning. The result should be a dataset (CSV or TSV), where each observation is described by one row.\n",
    "Since scikit-learn only works with numerical data, something must be done with the non-numerical data.\n",
    "\n",
    "Ensure the preprocessing is reproducible on both the training and test datasets, so that you can repeat the process multiple times as needed (iteratively).\n",
    "\n",
    "Because preprocessing can change the shape and characteristics of the data, you may need to perform EDA (Exploratory Data Analysis) again as necessary. These techniques will not be graded again, but document any changes in the chosen methods.\n",
    "You can solve data-related issues iteratively across all phases, as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b823b-ecb7-467d-abf2-2af9d3700a6c",
   "metadata": {},
   "source": [
    "## 2.1 Implementation of Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bb233-dc76-4391-b893-beeb833cb867",
   "metadata": {},
   "source": [
    "### A - Train–Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae639950-ff60-4e42-8b56-ead60d49b0d5",
   "metadata": {},
   "source": [
    "### B - Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0307e7-086a-42b6-a078-9a01d1619b39",
   "metadata": {},
   "source": [
    "### C - Feature Scaling and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f8b512-ae65-4923-b382-c2cba6c23ceb",
   "metadata": {},
   "source": [
    "### D - Justification and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b9339e-5c24-414f-858d-500c944d687d",
   "metadata": {},
   "source": [
    "## 2.2 Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7379fb-af8d-4059-b891-aa7a5472f972",
   "metadata": {},
   "source": [
    "### A - Identification of Informative Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa483780-560a-4e32-836b-05c3d640d417",
   "metadata": {},
   "source": [
    "### B - Ranking of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec93b5-049e-40cb-940c-671d53552f10",
   "metadata": {},
   "source": [
    "### C - Justification and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff557b-b0d8-45dd-939b-f0c7e55a3eab",
   "metadata": {},
   "source": [
    "## 2.3 Reproducibility of Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b190199a-1b7d-41fc-9d50-62918d367384",
   "metadata": {},
   "source": [
    "### A - Code Generalization for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d0ee9e2-19a1-4f30-be0c-08a678a5e088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d66260b4-7e25-4b40-9805-3c2e56f235ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Preprocess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f8d6dab-92de-45e0-ad25-016588b588ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class ExtractFeatures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42d793-a199-47ad-88f0-33fae40e30cc",
   "metadata": {},
   "source": [
    "### B - Pipeline Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61e8a948-0b4b-47d3-b769-839b1172ec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def enforce_schema(df, schema):\n",
    "    if schema is None:\n",
    "        return df \n",
    "        \n",
    "    \n",
    "    df = df.copy()\n",
    "\n",
    "    for col, col_type in schema.items():\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if col_type == 'int':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype(int)\n",
    "\n",
    "        elif col_type == 'float':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype(float)\n",
    "\n",
    "        elif col_type == 'numeric':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "\n",
    "        elif col_type == 'date':\n",
    "            df[col] = df[col].apply(\n",
    "                lambda x: dateparser.parse(str(x))\n",
    "                if pd.notnull(x) else pd.NaT\n",
    "            )\n",
    "\n",
    "        elif col_type == 'string':\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col] = df[col].replace('nan', np.nan)\n",
    "\n",
    "        elif col_type == 'bool':\n",
    "            df[col] = (\n",
    "                df[col]\n",
    "                .astype(str)\n",
    "                .str.lower()\n",
    "                .map({'true': True, 'false': False, '1': True, '0': False})\n",
    "            )\n",
    "\n",
    "        elif col_type == 'category':\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "187356c7-ec03-4ec0-b37b-5acdc7cc0bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_schema = {\n",
    "    'location':'string',\n",
    "    'code':'string',\n",
    "    'revision':'date',\n",
    "    'station':'string',\n",
    "    'latitude':'float',\n",
    "    'longitude':'float',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e71ec143-4e6b-499e-a85d-8a8f0d1c5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_schema = {\n",
    "    'SpO₂':'float',\n",
    "    'HR':'float',\n",
    "    'PI':'float',\n",
    "    'RR':'float',\n",
    "    'EtCO₂':'float',\n",
    "    'FiO₂':'float',\n",
    "    'PRV':'float',\n",
    "    'BP':'float',\n",
    "    'Skin Temperature':'float',\n",
    "    'Motion/Activity index':'float',\n",
    "    'PVI':'float',\n",
    "    'Hb level':'float',\n",
    "    'SV':'float',\n",
    "    'CO':'float',\n",
    "    'Blood Flow Index':'float',\n",
    "    'PPG waveform features':'float',\n",
    "    'Signal Quality Index':'float',\n",
    "    'Respiratory effort':'float',\n",
    "    'O₂ extraction ratio':'float',\n",
    "    'SNR':'float',\n",
    "    'oximetry':'int',\n",
    "    'latitude':'float',\n",
    "    'longitude':'float'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ca200582-7107-40f0-94b3-4a37e746a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = pd.read_csv(\"dataset/observation.csv\", sep='\\t')\n",
    "enforce_schema(observation, observation_schema)\n",
    "station = pd.read_csv(\"dataset/station.csv\", sep='\\t')\n",
    "enforce_schema(station, station_schema)\n",
    "patient = pd.read_csv(\"dataset/patient.csv\", sep='\\t')\n",
    "enforce_schema(patient, patient_schema)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7822946-896f-4dea-ad7c-3bcdf98f5a67",
   "metadata": {},
   "source": [
    "1. pred rozdelenim: schema ,odstranim logickych outlierov, nespojim tabulky, removnem riadok ak nemam oxymetry, odstranenie irelevantnych stlpcov, duplikaty\n",
    "2. rozdelenie dat na trenovacie a testovacie\n",
    "3. zadefinovanie pipeline\n",
    "4. fit na trenovacich datach\n",
    "5. predict na testovacich datach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
